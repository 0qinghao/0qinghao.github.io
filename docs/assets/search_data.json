

[
  
  
    {
      "title"    : "页面没有找到",
      "url"      : "https://0qinghao.github.io/inforest/404.html"
    } ,
  
  
  
    {
      "title"    : "About",
      "url"      : "https://0qinghao.github.io/inforest/about/"
    } ,
  
  
  
    {
      "title"    : "归档",
      "url"      : "https://0qinghao.github.io/inforest/archives/"
    } ,
  
  
  
    {
      "title"    : "Categories",
      "url"      : "https://0qinghao.github.io/inforest/categories/"
    } ,
  
  
  
  
  
    {
      "title"    : "Links",
      "url"      : "https://0qinghao.github.io/inforest/links/"
    } ,
  
  
  
    {
      "title"    : "Open Source Projects",
      "url"      : "https://0qinghao.github.io/inforest/open-source/"
    } ,
  
  
  
  
  
    {
      "title"    : "Wiki",
      "url"      : "https://0qinghao.github.io/inforest/wiki/"
    } ,
  
  
  
  
  
  
  
  
  
  
  
  

  
    {
      "title"    : "树莓派学习手记——修改软件源",
      "category" : "raspberrypi",
      "content": "国情，国情 在Raspbian/Ubuntu系统上，升级系统或安装软件只需要一条很简单的命令： sudo apt install 软件包名 t t# 安装软件 sudo apt upgrade t t# 更新软件 然而在天朝的网络下，很难顺利地完成下载过程。但好在有许多高校/机构提供了及时更新的镜像网站，我们可以通过修改配置文件解决下载难的问题。 很多同学查找解决方法后，或许能解决一部分问题，但仍会遇到连接超时的问题。究其原因，大致有两点：  树莓派的软件源配置有两处，而大部分教程只指出了一处；   没有区分系统版本（Codename），Codename目前分为jessie / wheezy / squeeze / stretch，大部分教程仍使用的是jessie或wheezy，而笔者安装的系统却是stretch。 ​ 配置文件在哪 /etc/apt/sources.list /etc/apt/sources.list.d/raspi.list 很多教程只指出了第一处，如果没有修改第二个配置文件，更新系统时很容易出现连接超时的问题。 在修改配置文件之前，可以选择先备份一下原文件，但这个配置文件也不太重要，不想麻烦也可跳过。 sudo cp /etc/apt/sources.list /etc/apt/sources.bak sudo cp /etc/apt/sources.list.d/raspi.list /etc/apt/sources.list.d/raspi.bak ​ 我的Codename是什么 我们来确定自己树莓派安装的系统Codename是什么： lsb_release -a 运行这条指令之后，可以很清楚的看到Codename Codename: stretch ​ 修改配置文件 国内有许多高校提供了树莓派的软件源镜像。可以在这个网页查看所有的镜像网站：http://www.raspbian.org/RaspbianMirrors 笔者选择了中科大提供的镜像，也是大家公认的比较稳定的镜像之一。 sudo nano /etc/apt/sources.list 将该文件的内容替换为： deb http://mirrors.ustc.edu.cn/raspbian/raspbian/ stretch main contrib non-free deb-src http://mirrors.ustc.edu.cn/raspbian/raspbian/ stretch main contrib non-free 按CTRL+X 关闭文件，键入Y（保存修改）回车。 修改第二个配置文件： sudo nano /etc/apt/sources.list.d/raspi.list 类似地，内容替换为： deb http://mirrors.ustc.edu.cn/archive.raspberrypi.org/ stretch main ui deb-src http://mirrors.ustc.edu.cn/archive.raspberrypi.org/ stretch main ui 相信细心的同学已经注意到了，修改的文件内容网址后紧接着一项”stretch”。如果你手中的树莓派安装的系统Codename并不是stretch，还请进行相应修改。 最后，刷新软件列表： sudo apt update 修改完成了！赶紧去体验一下高速更新系统/升级软件的快感吧。感谢你阅读文章！ ",
      "url"      : "https://0qinghao.github.io/inforest/2018/01/06/RPi-apt-source/"
    } ,
  
    {
      "title"    : "申请和使用Google云计算引擎配置SS Server",
      "category" : "google cloud",
      "content": "Google Cloud Platform的新用户可以获得$300赠金的一年使用权，使用这笔不菲的赠金，我们可以构建应用程序、搭建网站、存储数据、体验各种强大的API。这次，我总结了使用Google云计算引擎搭建SS服务器，实现科学上网的过程，也算作为墙内使用谷歌云平台的第一步。 科学上网的基本原理 我们只需要一个能够访问墙外目标地址的代理服务器。本地设备向服务器发送访问目标地址的请求，服务器收到请求后访问目标并将结果回传给本地设备。 我们是使用Shadowsocks（简称SS，中文名影梭）来配置服务器的，所以一般把这个服务器称为SS服务器。谷歌云平台提供的位于国外的云计算引擎可以用来搭建SS服务器。 申请试用谷歌云平台 *重要：你需要一张外币信用卡（VISA/MasterCard/JCB） 首先，翻墙。突然有种鸡生蛋，蛋生鸡的矛盾，不过我相信你能找到一个免费试用的VPN。 登录谷歌云平台，点击右上角的申请试用后进入申请界面。地区可以选择中国，不影响后续的申请。 账号类型选择“个人”，填写名称地址电话。 付款方式填写你的外币信用卡（单币银联卡无效）。提交后信用卡会扣除1美金进行验证，验证完成即退回。 创建计算引擎 进入控制台，首先要求创建一个项目，尽量使用简单易记的项目名。 项目创建完成后，点击控制台左上角的 ☰ 打开导航栏，找到 Compute Engine → VM实例 ，点击 创建 开始创建一个计算引擎。 区域 有3个比较好的选择： asia-east1：位于台湾 asia-southeast1：位于新加坡 asia-northeast1：位于东京 从国内ping延迟都在100ms左右，它们的流量费用和硬件费用有细微的差别，在意的朋友可以在这里查询。 机器类型 可以选择最小的微型（1个共享vCPU，0.6GB内存）以节省硬件费用，单作为SS服务器该配置已经足够。 其他设置可以保持默认。点击 创建 。 配置SS服务器 创建完成后可以看到分配给实例的 外部IP ，请牢记。 点击云引擎后面的 SSH ，远程连接该主机，进行配置。 这里使用秋水逸冰大大的SS服务器配置脚本。 依次输入下面三条指令： wget --no-check-certificate -O shadowsocks.sh https://raw.githubusercontent.com/wjk199511140034/ss-onekeyinstall/master/shadowsocks.sh sudo chmod +x shadowsocks.sh sudo ./shadowsocks.sh 2&gt;&amp;1 | tee shadowsocks.log 第三条指令运行后即进入配置过程，需要根据提示输入几项信息。 Please input password for shadowsocks-libev：输入 密码 ，请牢记 Please enter a port for shadowsocks-libev：输入SS 服务器端口号 ，请牢记 Which cipher you’d select：选择一种 加密方式 ，请牢记 按任意键开始执行脚本，等待脚本运行完毕。 创建防火墙规则 点击控制台左上角的 ☰ 打开导航栏，找到 VPC网络 → 防火墙规则 ，点击 创建防火墙规则 创建如下2个规则。  入站规则 流量方向 ：入站 目标 ：网络中的所有实例 来源 IP 地址范围 ：0.0.0.0/0 协议和端口 ：全部允许 其他部分可以保持默认，这条规则表示允许所有ip/端口的所有协议入站。  出站规则 流量方向 ：出站 目标 ：网络中的所有实例 来源 IP 地址范围 ：0.0.0.0/0 协议和端口 ：全部允许 其他部分可以保持默认，这条规则表示允许所有协议出站到所有ip/端口。 至此，SS服务器部署完毕。你可以关闭你不稳定的试用版VPN，准备开始正确地科学上网了。 使用SS客户端 这里仅以Windows客户端为例，Android端很相似。Debian平台使用SS客户端则需要进行一些配置，将另外做一次总结。 可以在GitHub下载到Windows平台的SS客户端。 如果你无法打开GitHub，可以点击这里，前往微云下载，但不保证是最新版本。 请将可执行程序放置在合适的文件夹内，运行后会在程序同一目录下产生配置文件，如果随便放置容易显得杂乱。 第一次打开SS客户端会主动要求编辑服务器。填入你的 外部IP 密码 服务器端口号 加密方式 ，其他设置可以保持默认。 最后，右击任务栏的小图标，勾选 启用系统代理 。系统代理模式选择 PAC模式 ，这样SS会自动使用代理访问墙外站点，不需要另外安装浏览器的代理插件。 参考资料 Debian下shadowsocks-libev一键安装脚本 Shadowsocks Troubleshooting Shadowsocks原理和搭建 Google Cloud服务免费申请试用以及使用教程 感谢你阅读文章！ ",
      "url"      : "https://0qinghao.github.io/inforest/2018/02/27/google-cloud-engine-ss-server/"
    } ,
  
    {
      "title"    : "Shadowsocks客户端在不同系统下的使用方法",
      "category" : "shadowsocks",
      "content": "当我们配置好Shadowsocks服务器端，或是购买了SS账号后，就可以使用客户端开始科学上网了。下面分别介绍在Windows、安卓、Linux(Debian)系统下SS客户端的使用方法。 Windows系统下SS客户端的使用方法 Windows系统下的SS客户端使用起来最为方便。客户端自带了系统全局代理的功能，甚至可以省去配置浏览器插件的操作。 下载客户端  点击这里，跳转到GitHub下载 如果你无法打开GitHub，可以点击这里，前往微云下载，但不保证是最新版本 将压缩包内的可执行程序解压，放置在合适的文件夹内，运行后会在程序同一目录下产生配置文件，如果随便放置容易显得杂乱。 配置客户端 第一次打开SS客户端会主动要求编辑服务器。填入你的 服务器地址 密码 服务器端口号 加密方式 ，其他设置可以保持默认。 最后，右击任务栏的小图标，勾选 启用系统代理 。系统代理模式选择 PAC模式 ，这样SS会自动使用代理访问墙外站点，不需要另外安装浏览器的代理插件。 安卓系统下SS客户端的使用方法 安卓系统下的SS客户端也很完善，配置方便，甚至还可以指定仅部分APP使用代理。 下载客户端  如果你能使用Google Play商店，直接搜索安装Shadowsocks 你也可以点击这里，前往微云下载，但不保证是最新版本 配置客户端 点击右上角的 + 选择 手动设置 ，填入你的 服务器地址 密码 服务器端口号 加密方式 ，其他设置可以保持默认。 还可以在配置中开启 分应用VPN 功能，来指定仅部分APP的流量进行代理；或者再打开 绕行模式 来指定部分APP的流量绕过代理。 Debian下SS客户端的使用方法 Linux下使用SS客户端要麻烦一些，一方面Linux下SS不带全局代理，需要搭配浏览器插件使用；另一方面笔者在使用中有遇到bug，不知在你阅读这篇文章时是否已经修复，总之还是会记录在下文中以供参考。 另外，这部分介绍的是配合Chrome插件实现浏览器翻墙的方法。关于如何在LX终端让 wget curl 等命令使用代理，将在另一篇文章中再做总结。 安装客户端 sudo apt update sudo apt install shadowsocks 运行sslocal 不带任何参数运行 sslocal 可以查看帮助。 运行SS客户端一般有两种方法。你可以参考帮助，将必要的参数填入，用一条较长的指令来运行： sudo sslocal -s 服务器地址 -p 服务器端口 -k 密码 -m 加密方式 -d start 显然上面这种方式效率太低。另一种方式就是将各项参数保存为json文件，运行时指定配置文件即可。 假设我们的配置文件是 /etc/ss.json ，其内容为： { server:服务器地址, server_port:服务器端口, local_address:127.0.0.1, local_port:1080, password:密码, timeout:600, method:加密方式, fast_open:false } 将你的 服务器地址 密码 服务器端口号 加密方式 替换到上述文件。（有双引号的请保留双引号，不要删除） 接下来，每次需要运行SS客户端时，我们只需要输入一条简短的指令： sudo sslocal -c /etc/ss.json -d start *运行sslocal时遇到的bug  解决方案来自Kali2.0 update到最新版本后安装shadowsocks服务报错问题 笔者在运行sslocal命令时遇到了形如 INFO loading libcrypto from libcrypto.so.1.1 的报错。后在上述文章中找到解决方案。 打开文件openssl.py，请参照错误提示确定是否与下述文件路径相同： sudo nano /usr/local/lib/python2.7/dist-packages/shadowsocks/crypto/openssl.py 使用快捷键CTRL+_选择跳转到第52行 将 libcrypto. EVP_CIPHER_CTX_ cleanup .argtypes = (c_void_p, ) 修改为 libcrypto. EVP_CIPHER_CTX_ reset .argtypes = (c_void_p, ) 同样地，跳转到第111行 将 libcrypto. EVP_CIPHER_CTX_ cleanup (self.ctx) 修改为 libcrypto. EVP_CIPHER_CTX_ reset (self._ctx) 按CTRL+X，Y保存退出。重新执行sslocal指令运行正常。 安装Chrome插件 由于Linux下的SS客户端不带全局代理功能，需要配合浏览器插件使用。这里只介绍Chrome插件的安装方法，火狐大体上类似。 如果你能够使用Chrome应用商店，搜索SwitchyOmega安装即可。你也可以点击这里通过微云下载crx文件，将其拖动到Chrome扩展程序页面完成安装。 点击选项，如下图配置SwitchyOmega。 代理协议 选择 SOCKS5 ；如果你在ss.json配置文件中修改过 local_port 参数，则这里 代理端口 必须与其一致，否则保持默认值1080即可。 最后，保存配置，点击SwitchyOmega图标切换到刚才配置好的情景模式。 结语 由于手上设备有限，没办法总结所有系统下的SS客户端使用方法。例如iOS系统下似乎是使用Big Boss源搜索ShadowSocks应用，但没法亲自尝试。有兴趣的朋友建议前往官网https://shadowsocks.org/（.com那个是出售SS服务的）进一步了解。 感谢你阅读文章！ ",
      "url"      : "https://0qinghao.github.io/inforest/2018/03/06/shadowsocks-clients/"
    } ,
  
    {
      "title"    : "在Python中使用谷歌Cloud Speech API将语音转换为文字（另一种方案）",
      "category" : "google cloud",
      "content": "在之前发布的使用谷歌Cloud Speech API将语音转换为文字一文中，我们实现了在控制台使用curl发送post请求，得到语音转文字的结果；而在Python中使用谷歌Cloud Speech API将语音转换为文字一文中，我们实现了安装Cloud Speech API客户端库，通过调用库函数得到语音转文字的结果。 如果你尝试过这两种方法，就会发现其实后者得到结果需要的时间要长一些（笔者使用这两种方法得到结果的耗时分别大约是5秒、7秒）。那么，有没有办法在python中像第一种方法那样，使用curl命令发送post请求呢。当然是可行的，所以今天我们将介绍在Python中使用Cloud Speech API将语音转换为文字的另一种方案，另外这次我们将把音频文件编码为base64嵌入到json请求文件中，省去了上传声音文件到Cloud Storage的步骤。 相关说明之类的在上面两篇文章里已经写了很多，这边就直接贴代码。 *使用python3 import json import urllib.request import base64 # ① api_url = https://speech.googleapis.com/v1beta1/speech:syncrecognize?key=你的API密钥 audio_file = open('/home/pi/chat/test-speech/output.wav', 'rb') audio_b64 = base64.b64encode(audio_file.read()) audio_b64str = audio_b64.decode() t# ② # print(type(audio_b64)) # print(type(audio_b64str)) audio_file.close() # ③ voice = { config: {  #encoding: WAV,  languageCode: cmn-Hans-CN }, audio: {  content: audio_b64str } } # 将字典格式的voice编码为utf8 voice = json.dumps(voice).encode('utf8') req = urllib.request.Request(api_url, data=voice, headers={'content-type': 'application/json'}) response = urllib.request.urlopen(req) response_str = response.read().decode('utf8') # ④ # print(response_str) response_dic = json.loads(response_str) transcript = response_dic['results'][0]['alternatives'][0]['transcript'] confidence = response_dic['results'][0]['alternatives'][0]['confidence'] print(transcript) print(confidence) 几点说明： 注释 ① ：请求API的链接，请替换 你的API密钥 。如果你有疑问，或许可以参考 创建API密钥 - 使用谷歌Cloud Speech API将语音转换为文字 。 audio_file 路径替换为你的本地声音文件路径。 注释 ② ：这次上传音频的方式是，将声音文件编码为base64，把对应的整个字符串放进json请求中。如果你执行 print(type(audio_b64)) 就会发现编码后的audio_b64是 bytes 类型，所以还需要做一次decode()，转成字符串。 注释 ③ ：先以字典格式保存json请求内容，代表声音文件的字符串就在这里放入。 注释 ④ ：API返回的结果保存在 response_str ，如果你直接运行 print(response_str) 就会发现这个字符串可以看做一个有很多“层”的字典，要提取出识别结果，需要搞清楚这个字典到底是怎么组成的： 第 1 层：花括号{}说明字符串 response_str 在执行 json.loads 后变成一个”字典”。得到”字典” response_dic 。 第 2 层：字典中只有一组键-值， response_dic['results'] 取出唯一的键”results”对应的值。观察这个值，发现中括号[]，说明这个值的类型是”列表“。 第 3 层：观察列表 response_dic['results'] ，发现列表中只有一项数据，但这项数据又是”字典”类型。将其取出，得到”字典” response_dic['results'][0] 。 第 4 层：字典中又是只有一组键-值， response_dic['results'][0]['alternatives'] 取出唯一的键”alternatives”对应的值。观察这个值，[]说明我们得到的结果又是一个”列表”。 第 5 层：观察列表 response_dic['results'][0]['alternatives'] ，列表中只有一项数据， response_dic['results'][0]['alternatives'][0] 再将这唯一一项数据取出，发现得到的是一个”字典”，而这次得到的字典中有两组键-值，分别取出就是我们要的结果和置信度了。 transcript = response_dic['results'][0]['alternatives'][0]['transcript'] confidence = response_dic['results'][0]['alternatives'][0]['confidence']  小结： 今天介绍的这种方案，获取结果需要的时间比用API客户端库要快一些，另外应用了把本地语音编码后放入json请求的方式，也能方便后期和录音程序结合在一起使用。但稍有一点缺点是API密钥直接暴露在代码中，对实际应用可能会有一些影响。 下一步的目标是和录音功能结合起来，实现自动识别当前录制的语音。 感谢你阅读文章！ ",
      "url"      : "https://0qinghao.github.io/inforest/2018/03/08/google-cloud-speech-api-voice2text-python-another-way/"
    } ,
  
    {
      "title"    : "在Python中使用谷歌Cloud Speech API将语音转换为文字",
      "category" : "google cloud",
      "content": "之前我们总结了使用谷歌Cloud Speech API将语音转换为文字的基本流程，然而那只是在命令行中使用curl实现的。这次我们将总结在Python中使用Cloud Speech API的方法。 配置Python开发环境 笔者使用的是树莓派（Debian）进行试验的，其他平台的配置方法可以在这里查找。 安装Python 大多数Linux发行版都包含Python。对于Debian和Ubuntu，运行以下指令确保Python版本是最新的： sudo apt update sudo apt install python python-dev python3 python3-dev python-pip python3-pip 安装和使用virtualenv 尽管这一步不是必须的，但强烈建议你使用virtualenv。virtualenv是一种创建独立Python环境的工具，可以将每个项目的依赖关系隔离开来。在虚拟环境下，你可以不必顾虑python2和python3的冲突；另外一个优势是可以直接将你的项目文件夹复制到其他机器上，文件夹内就包含了项目所依赖的软件包。 sudo apt install python-virtualenv 安装完成后，就可以在你的项目文件夹中创建一个虚拟环境。 cd 项目文件夹 virtualenv --python python3 env 使用 --python 标志来告诉virtualenv要使用哪个Python版本，这次试验将全程以python3环境进行。执行后会在 项目文件夹 内创建一个 env 文件夹。 创建完成后，你需要“激活”virtualenv。激活virtualenv会告诉你的shell为Python使用virtualenv的路径。 source env/bin/activate 看到激活虚拟环境后，你就可以放心地安装软件包，并确信它们不会影响其他项目。 如果你想停止使用virtualenv并返回到全局Python环境，你可以关闭它： deactivate 配置Cloud Speech API客户端库 我们假定你已经有合适的代理，能够使用谷歌服务，并且已经开始使用Google云平台。如果你有疑问，或许可以参考这篇文章。 安装客户端库 如果你安装了virtualenv，请确保激活了虚拟环境。 pip install --upgrade google-cloud-speech 值得一提的是，笔者使用的树莓派在安装进行到 Running setup.py bdist_wheel for grpcio ... 时停留了非常久（10分钟以上），这属于正常现象，树莓派编译进行得很慢，需要耐心等待。 设置验证 登录谷歌云平台控制台，前往创建服务账号密钥界面。 从 服务帐户 下拉列表中选择 新建服务帐户 。输入合适的 服务帐号名称 ， 角色 选择 Project → 所有者 。 密钥类型 选择 JSON 。 点击 创建 后，会开始下载包含密钥的JSON文件，请妥善保存 。 最后，将环境变量 GOOGLE_APPLICATION_CREDENTIALS 设置为含密钥的JSON文件的文件路径，例如： export GOOGLE_APPLICATION_CREDENTIALS=/home/pi/speech/speech-account.json 请将 /home/pi/speech/speech-account.json 替换为你的json文件路径。 当然，直接输入上述命令设置的环境变量是临时的。一个比较实用的方法是在 ~/.bashrc 文件中设置环境，之后就不需要再手动设置了。 sudo nano ~/.bashrc 在文件末尾插入上述 export 命令，保存。 使用客户端库 下例给出了使用客户端库的方法。 import io import os # Imports the Google Cloud client library from google.cloud import speech from google.cloud.speech import enums from google.cloud.speech import types # Instantiates a client client = speech.SpeechClient() # The name of the audio file to transcribe file_name = os.path.join(  os.path.dirname(__file__),  'voice.wav') # Loads the audio into memory with io.open(file_name, 'rb') as audio_file:  content = audio_file.read()  audio = types.RecognitionAudio(content=content) config = types.RecognitionConfig(  encoding=enums.RecognitionConfig.AudioEncoding.LINEAR16,  sample_rate_hertz=16000,  language_code='cmn-Hans-CN') # Detects speech in the audio file response = client.recognize(config, audio) for result in response.results:  print('Transcript: {}'.format(result.alternatives[0].transcript))  print('Confidence: {}'.format(result.alternatives[0].confidence)) 几点说明： file_name 给出了声音文件的路径。其中 os.path.dirname(__file__) 表示py代码所在文件夹的路径。故上例中声音文件是py代码相同目录下的 voice.wav 。 config 给出了声音文件的编码信息，Cloud Speech API并不支持任意格式的声音文件，详细要求参见：AudioEncoding - Google Cloud Speech API 。 language_code='cmn-Hans-CN' 表示识别语言为中文普通话。常用的还有American English ( en-US )、British English ( en-GB )、日本語( ja-JP )、廣東話( yue-Hant-HK )。更多语言支持可以在Language Support - Google Cloud Speech API查询。 运行结果： “Confidence”是置信度，越接近1准确性越高。 小结 至此，Cloud Speech API的使用总结就告一段落了，希望能对你有所帮助。这篇总结是参照着Google Cloud Speech API文档写下的，如果有何纰漏恳请指出。 感谢你阅读文章！ ",
      "url"      : "https://0qinghao.github.io/inforest/2018/03/08/google-cloud-speech-api-voice2text-python/"
    } ,
  
    {
      "title"    : "使用谷歌Cloud Speech API将语音转换为文字",
      "category" : "google cloud",
      "content": "Google Cloud Speech API是由谷歌云平台提供的，利用机器学习技术将语音转换为文字的服务。这个API能识别超过80种语言和语言变体，包括中文、日语、英语甚至广东话。这次，我总结了使用Google Cloud Speech API的基本流程。 花5秒钟试用Cloud Speech API吧 在Cloud Speech API概览页，我们可以体验将语音转换为文字的效果。只需要选择一种语言即可开始使用，甚至不需要登录谷歌账号。（加载出来需要一些时间） 在项目中添加API 使用Cloud Speech API需要登录谷歌云平台并申请免费试用，申请试用谷歌云平台的流程可以参考这篇文章 。 我们假定你能够使用谷歌云平台，并且已经创建了一个项目，下面介绍如何把Cloud Speech API添加到项目中。 点击控制台左上角的 ☰ 打开导航栏，找到 API和服务 → 库 。 在搜索框中键入 Speech 即可找到 Cloud Speech API 。 打开API页面，点击 启用 。 创建API密钥 回到之前的页面，选择 凭据 → 创建凭据 → API密钥 。 马上 API密钥 就创建好了，虽然随时都能在这个页面查询，但为了方便起见，将其记录下来备用吧，很快就要用到它。 限制密钥 选项默认情况下应该是“无”，这次只是试着使用API，保持默认“无”即可。 准备声音文件 虽然有些麻烦，但是接下来我们要准备声音文件。Cloud Speech API没办法直接识别mp3、mp4中的声音，我们需要准备FLAC、WAV格式的音频。而且仅支持单声道音频，所以一般都需要转码之类的工作。 详细的声音文件要求参见：AudioEncoding - Google Cloud Speech API 基于上述情况，我读了下面这段文稿，并制成了FLAC格式（单声道）的声音文件。是用手机麦克风进行录音的，质量一般(´・ω・｀) 是否可以正确识别呢？ voice.flac  寄蜉蝣于天地，渺沧海之一粟。哀吾生之须臾，羡长江之无穷。挟飞仙以遨游，抱明月而长终。 《赤壁赋》 将声音文件上传到Cloud Storage 如果要使用Cloud Speech API识别本地声音文件，必须将音频文件编码为base64，然后嵌入到稍后将创建的json请求文件中，这虽然可行但并不方便。如果你想使用这种方法，请参考：Embedding Base64 encoded audio - Google Cloud Speech API 我们将使用另一种方案，将声音文件上传到Google Cloud Storage。 点击控制台左上角的 ☰ 打开导航栏，找到 存储 → 浏览器 。 点击 创建存储分区 。 输入合适的 存储分区名称，后文将要用到。默认存储类别选择”Multi-Regional”，Multi-Regional位置选择”亚洲”。点击 创建 。 点击 上传文件 ，上传声音文件，勾选 公开链接 。（该音频将能被任何人访问，请注意）  2018年10月18日更新： 刚看了一下，页面有所改变，暂时没找到公开单个音频文件的方法。 你可以这样做，把整个存储分区公开： 导航栏→存储→浏览器→存储分区最后有个选项，点开来→修改存储分区权限→“添加成员”填“allUsers”，“角色”选“存储对象查看者”→添加 注意：这样该分区内所有内容都可能被任何人访问到 另外，请记住上传文件的 文件名 ，后文将用到。 将语音转换为文字 终于，可以使用Cloud Speech API将语音转换为文字了。 首先，我们新建一个json格式的请求文件（request.json）。文件名无特殊要求。 { config: {  encoding:FLAC,  languageCode:cmn-Hans-CN }, audio: {  uri:gs://存储分区名称/文件名 } } 注意3个地方： cmn-Hans-CN ：表示识别语言为中文普通话。常用的还有American English ( en-US )、British English ( en-GB )、日本語( ja-JP )、廣東話( yue-Hant-HK )。更多语言支持可以在Language Support - Google Cloud Speech API查询。 存储分区名称 ：刚才是否有记录下来呢？如果没有记住可以点击控制台左上角的 ☰ 打开导航栏，找到 存储 → 浏览器 查看。 文件名 ：存储在Cloud Storage中的音频文件名，可以在存储分区中查看。 最后，我们使用curl命令（Windows平台需另外安装）向Cloud Speech API发出请求。 cd到json请求文件所在目录。  curl -H “Content-Type: application/json” -d @request.json “https://speech.googleapis.com/v1/speech:recognize?key=API密钥” 注意2个加粗处： request.json ：json请求文件的文件名。 API密钥 ：替换为你记录下来的API密钥。如果没有记下来，可以点击控制台左上角的 ☰ 打开导航栏，找到 API和服务 → 凭据 查看。 得到结果： 可以看到返回结果也是json格式的数据。”confidence”是置信度，越接近1准确性越高。 小结 第一次尝试语音识别服务，得到结果的时候很开心。或许有人会惊讶上例语音识别的准确性，但正如文章开头所说“Cloud Speech API是利用机器学习技术将语音转换为文字的服务”，像上例中这样的俗语、名著甚至是歌词，准确率都出奇地高。如果你录制一段日常语音交给Cloud Speech API识别，结果就不那么满意了。 最后，这次只是使用curl命令在LX终端获得了识别结果，下次将会总结如何在编程语言中使用Cloud Speech API。 感谢你阅读文章！ ",
      "url"      : "https://0qinghao.github.io/inforest/2018/03/08/google-cloud-speech-api-voice2text/"
    } ,
  
    {
      "title"    : "树莓派学习手记——使用Python录音",
      "category" : "raspberrypi",
      "content": "有的时候我们想让树莓派能够录音，以实现语音控制等功能。所以今天我们总结一下用在树莓派上使用Python录音的过程。 准备硬件 树莓派上自带的3.5mm接口只能作为语音输出口，不能接麦克风。所以我们需要另外购买USB声卡，某宝上5元左右就能买到，当然你还需要一个麦克风。总费用应该在20元以内。 检查硬件是否正常 使用arecord -l可以列出所有录音设备，一般输出如下： arecord -l List of CAPTURE Hardware Devices card 1: Device [USB Audio Device], device 0: USB Audio [USB Audio] Subdevices: 1/1 Subdevice #0: subdevice #0 同样地，aplay -l可以列出所有播放设备，输出中也能找到形如 Device [USB Audio Device] 的设备。 我们可以直接在命令行执行Linux自带的录音/播放命令，测试硬件是否正常： arecord -D hw:1,0 -t wav -c 1 -r 44100 -f S16_LE test.wav aplay -D hw:0,0 test.wav arecord 是录音命令，其中 hw:1,0 表示 card 1: Device [USB Audio Device], device 0: USB Audio [USB Audio] 的 card 1 , device 0 ，如果你的USB声卡录音设备不是 card 1 , device 0 ，还请进行相应修改。另外，录音过程需要手动按CTRL + C结束。 aplay 是播放命令，其中 hw:0,0 表示树莓派板载音频接口，如果你把耳机插在USB声卡接口，还请进行相应修改，如改成 hw:1,0 。 如果你发现录制的音频内没有声音，只有细微的杂音，但 arecord -l 和 aplay -l 列出的设备中确实有USB声卡。那么你可以尝试着把麦克风接口拔出来一些，只插进去2/3，或许能够解决你的问题。笔者不是很明白其中的缘由，如果你有什么想法恳请留言告知。 安装pyaudio 在Python中执行录音命令需要pyaudio模块，直接用pip命令安装： pip install pyaudio 如果你使用pip命令下载速度很慢，或许修改pip源可以帮到你。 如果你使用了virtualenv，一般会发现pyaudio安装失败。这种情况下你需要安装APT中的PortAudio开发头文件，然后安装PyAudio： sudo apt-get install portaudio19-dev pip install pyaudio 使用Python录音 该例程修改自官方主页例程PyAudio。 import pyaudio import wave import os import sys CHUNK = 512 FORMAT = pyaudio.paInt16 CHANNELS = 1 RATE = 44100 RECORD_SECONDS = 5 WAVE_OUTPUT_FILENAME = output.wav p = pyaudio.PyAudio() stream = p.open(format=FORMAT,    channels=CHANNELS,    rate=RATE,    input=True,    frames_per_buffer=CHUNK) print(recording...) frames = [] for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):  data = stream.read(CHUNK)  frames.append(data) print(done) stream.stop_stream() stream.close() p.terminate() wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb') wf.setnchannels(CHANNELS) wf.setsampwidth(p.get_sample_size(FORMAT)) wf.setframerate(RATE) wf.writeframes(b''.join(frames)) wf.close() 执行后会录制一段5秒的音频，输出为同目录下的output.wav文件。 python3 rec.py * 隐藏错误消息 一般情况下，在树莓派上执行上述Python代码后，你会看到非常多的ALSA报错和JACK报错：  ALSA lib confmisc.c:1281:(snd_func_refer) Unable to find definition ‘cards.bcm2835.pcm.front.0: CARD=0’ …… …… connect(2) call to /tmp/jack-1000/default/jack_0 failed (err=No such file or directory) attempt to connect to server failed 但你会发现其实能够正常地录音。如果你不想看到这些错误消息，可以在代码中加入下述命令隐藏错误： os.close(sys.stderr.fileno()) 小结 使用Python录音很简单，你还可以在GPIO口上接入一个按钮，修改例程，实现按下按钮自动开始录音的功能。下一步的目标是把Python录音和Cloud Speech API语音识别结合起来。 感谢你阅读文章！ ",
      "url"      : "https://0qinghao.github.io/inforest/2018/03/20/RPi-recorder-python/"
    } ,
  
    {
      "title"    : "使用Python与图灵机器人聊天",
      "category" : "python",
      "content": "图灵机器人对中文的识别准确率高达90%，是目前中文语境下智能度最高的机器人。有很多在Python中使用图灵机器人API的博客，但都是1.0版本。所以今天简单地总结一下在Python中使用图灵机器人API v2.0的方法。 获取API KEY 首先，前往图灵机器人官方网站 http://www.tuling123.com/ 注册账号。 登录后点击 创建机器人 ，填写一些简单的基本信息之后即可创建。 在机器人设置界面找到你的 API KEY ，记录下来。 在Python中使用图灵机器人API v2.0 基本原理就是使用urllib.request模块，向接口地址发送HTTP POST请求，请求中加入了聊天内容。 *使用python3执行 import json import urllib.request api_url = http://openapi.tuling123.com/openapi/api/v2 text_input = input('我：') req = {  tperception:  t{  t tinputText:  t t{  t t ttext: text_input  t t},  t tselfInfo:  t t{  t t tlocation:  t t t{  t t t tcity: 上海,  t t t tprovince: 上海,  t t t tstreet: 文汇路  t t t}  t t}  t},  tuserInfo:  t{  t tapiKey: 请替换为你的API KEY,  t tuserId: OnlyUseAlphabet  t} } # print(req) # 将字典格式的req编码为utf8 req = json.dumps(req).encode('utf8') # print(req) http_post = urllib.request.Request(api_url, data=req, headers={'content-type': 'application/json'}) response = urllib.request.urlopen(http_post) response_str = response.read().decode('utf8') # print(response_str) response_dic = json.loads(response_str) # print(response_dic) intent_code = response_dic['intent']['code'] results_text = response_dic['results'][0]['values']['text'] print('Turing的回答：') print('code：' + str(intent_code)) print('text：' + results_text)  几点说明： 1、字典 req 包含了向图灵机器人发出请求所需的各项信息。其中 req['perception']['selfInfo']['location'] 包含了地理位置信息，向图灵机器人发送与位置有关的请求时，如果没有另外指定位置，则会默认使用这个位置。例如询问”明天会下雨吗”，图灵机器人会回答我”上海”明天是否下雨。 2、 req['userInfo'] 包含了API KEY，请替换成你的API KEY（双引号不要删除）。另外 userId 是用户参数，暂时不明白用途，如果你有什么想法恳请留言。 3、图灵机器人的回答可以转换为python的字典格式。其中有一项 response_dic['intent']['code'] 官方称为”输出功能code”，表示这个回答是什么”类型”的。例如10004代表普通的聊天回复，10008代表与天气相关的回复。然而奇怪的是，目前API v2.0的官方文档并没有给出code和类型的对照表。目前自己总结了一些如下，欢迎补充：    code  类型     10004  聊天    10008  天气    10013  科普类，例如”班戟是什么”    10015  菜谱类，例如”剁椒鱼头怎么做”    10019  日期类，例如”愚人节是几号”、”明天是星期几”    10020  中英翻译    10023  一般返回网页会是这个code，例如”iphone多少钱”    10034  语料库中自己设定的回答   小结 到现在为止，已经快把每个独立的模块完成了，接下来该准备考虑如何把它们整合在一起了。希望能帮到你。 感谢你阅读文章！ ",
      "url"      : "https://0qinghao.github.io/inforest/2018/03/22/turing-chat-bot-python-API/"
    } ,
  
    {
      "title"    : "在Python中使用科大讯飞Web API进行语音合成",
      "category" : "python",
      "content": "前几日讯飞开放平台推出了WebAPI接口，恰好最近需要实现一个文字转语音的功能，于是就尝试着用了起来。但不知什么原因，官方文档的调用示例一直报错，最后自己照着示例的思路用python3重写了一遍。所以这次总结一下在Python中使用讯飞Web API进行语音合成的过程。 注册讯飞开放平台 首先注册讯飞开放平台：http://passport.xfyun.cn/register 注册完成后进入控制台，在控制台创建一个新应用 ，填写一些基本信息，注意 应用平台 选择 WebAPI 。 创建完成后，记录下 APPID 和 APIKey ，将在程序中用到。 另外，请在 IP白名单 中添加自己的外网IP，可以在http://www.ip138.com/ 查看。（一般来说外网IP会常常发生变化，请注意） 在Python3中使用讯飞Web API 先上代码，后面进行必要的说明： 可能提示缺库：pip3 install requests *使用python3执行 import base64 import json import time import hashlib import requests # API请求地址、API KEY、APP ID等参数，提前填好备用 api_url = http://api.xfyun.cn/v1/service/v1/tts API_KEY = 替换成你的APIKEY APP_ID = 替换成你的APPID OUTPUT_FILE = C://output.mp3 # 输出音频的保存路径，请根据自己的情况替换 TEXT = 苟利国家生死以，岂因祸福避趋之 # 构造输出音频配置参数 Param = {  auf: audio/L16;rate=16000, #音频采样率  aue: lame, #音频编码，raw(生成wav)或lame(生成mp3)  voice_name: xiaoyan,  speed: 50, #语速[0,100]  volume: 77, #音量[0,100]  pitch: 50, #音高[0,100]  engine_type: aisound #引擎类型。aisound（普通效果），intp65（中文），intp65_en（英文） } # 配置参数编码为base64字符串，过程：字典→明文字符串→utf8编码→base64(bytes)→base64字符串 Param_str = json.dumps(Param) #得到明文字符串 Param_utf8 = Param_str.encode('utf8') #得到utf8编码(bytes类型) Param_b64 = base64.b64encode(Param_utf8) #得到base64编码(bytes类型) Param_b64str = Param_b64.decode('utf8') #得到base64字符串 # 构造HTTP请求的头部 time_now = str(int(time.time())) checksum = (API_KEY + time_now + Param_b64str).encode('utf8') checksum_md5 = hashlib.md5(checksum).hexdigest() header = {  X-Appid: APP_ID,  X-CurTime: time_now,  X-Param: Param_b64str,  X-CheckSum: checksum_md5 } # 发送HTTP POST请求 def getBody(text):  data = {'text':text}  return data response = requests.post(api_url, data=getBody(TEXT), headers=header) # 读取结果 response_head = response.headers['Content-Type'] if(response_head == audio/mpeg):  out_file = open(OUTPUT_FILE, 'wb')  data = response.content # a 'bytes' object  out_file.write(data)  out_file.close()  print('输出文件: ' + OUTPUT_FILE) else:  print(response.read().decode('utf8')) 下面按照代码顺序进行各部分的说明。 APIKey等参数 在代码开头填好各项参数，方面代码中使用。 API_KEY和APP_ID请替换为上一步创建应用后得到的内容。请不要删除双引号。 OUTPUT_FILE是最终输出音频的保存路径，根据自己的情况替换。 TEXT是将要输出为语音的文本。 音频配置参数 Param 是字典格式的音频配置参数，其中 aue 可选 raw (生成wav)或 lame (生成mp3)，如果修改成raw请记得同时修改输出文件的扩展名。 最后需要将配置参数编码为Base64字符串：字典类型→明文字符串→utf8编码→Base64(bytes)→Base64字符串，具体实现可以参考代码。 音频配置参数的详细说明可以参考请求参数 - 语音合成 。 HTTP请求头部 根据 授权认证 - 科大讯飞RESET_API开发指南 ，在调用所有业务接口时，都需要在HTTP请求头部中配置以下参数用于授权认证：    参数  格式  说明     X-Appid  string  讯飞开放平台注册申请应用的应用ID(appid)    X-CurTime  string  当前UTC时间戳，从1970年1月1日0点0 分0 秒开始到现在的秒数    X-Param  string  音频配置参数JSON串经Base64编码后的字符串    X-CheckSum  string  令牌，计算方法：MD5(apiKey + curTime + param)。三个值拼接的字符串，进行MD5哈希计算（32位小写）。   具体实现参考代码中字典 header 。 发送请求&amp;读取结果 最后使用requests库发送HTTP POST请求，得到结果。根据响应的 header 可以判断是否合成成功。 若响应头部包含 Content-type: audio/mpeg ，则响应Body为音频数据，可写入文件保存。 若合成出现错误，响应头部包含 Content-type: text/plain ，响应Body为记载了错误类型的json字符串。 返回值的具体说明请参考 返回值 - 语音合成 。 运行结果 使用几次后，感觉合成语音的断句做得不是很优秀，但响应速度很快，还是比较满意的。 output.mp3 小结 最近使用了几种Web API，对这类API的使用方法也算是有些经验了。最后，现在语音识别、图灵机器人、语音合成都试着做了一遍，下一篇博客将把他们组合起来，实现一个简单的语音助手。 感谢你阅读文章！ ",
      "url"      : "https://0qinghao.github.io/inforest/2018/03/24/xunfei-tts-web-api-python/"
    } ,
  
    {
      "title"    : "使用Python把树莓派改造成一个语音助手",
      "category" : "python",
      "content": "语音助手已经不是什么新事物了。就在两三年前，语音助手的使用体验还不是那么好，尝尝鲜后也就没用过了。但最近发现不管是微软的Cortana、苹果的Siri，还是一些不怎么有名气的，例如MIUI的小爱同学等，使用体验真的改善了很多，确确实实能带来一些方便了。 随着各种云服务、API的面世，语音方面的云服务可以说是十分健全了。你是否也想过自己动手搭建一个语音助手系统呢？本文将总结使用Python把 树莓派（3代b型） 改造成一个简易语音助手的基本流程。 概述 这次要做的说白了，就是把各种云服务、API串起来，并不涉及任何核心技术、算法的实现，望知悉。 这次将要使用到的服务包括：  谷歌Cloud Speech API 图灵机器人 科大讯飞 语音合成WebAPI 为了实现这个语音助手系统，需要完成的工作每一个都不难，但数量稍多了些。以下是涉及到的一些博客：  使用Google云计算引擎实现科学上网 在Windows命令行、Linux终端使用代理 树莓派学习手记——使用Python录音 在Python中使用谷歌Cloud Speech API将语音转换为文字（另一种方案） 使用Python与图灵机器人聊天 在Python中使用科大讯飞Web API进行语音合成 后文在介绍各部分的具体实现时，只附上代码和进行一些必要的说明，详细内容还需要参考相应博客。 各部分的实现 由于整个项目用到的服务比较多，而且各部分的分工很明显，所以选择各部分分别用一个python程序来实现，最后再用一个程序整合在一起的方式。 录音 参考：树莓派学习手记——使用Python录音 笔者采用了“按住按钮进行录音”的操作方式，如下图所示接线。如果你手头上没有按钮或觉得这么做不方便，可以修改代码改成“按回车键开始/结束录音”之类的操作方式。 另外，树莓派的板载3.5mm耳机接口是不带语音输入功能的，所以你需要另外购买USB声卡。  文件 rec.py import RPi.GPIO as GPIO import pyaudio import wave import os import sys def rec_fun():  t# 隐藏错误消息，因为会有一堆ALSA和JACK错误消息，但其实能正常录音  tos.close(sys.stderr.fileno())  t  tBUTT = 26 t# 开始录音的按钮：一边接GPIO26，一边接地  tGPIO.setmode(GPIO.BCM)  t# 设GPIO26脚为输入脚，电平拉高，也就是说26脚一旦读到低电平，说明按了按钮  tGPIO.setup(BUTT, GPIO.IN, pull_up_down = GPIO.PUD_UP)  t# wav文件是由若干个CHUNK组成的，CHUNK我们就理解成数据包或者数据片段。  tCHUNK = 512  tFORMAT = pyaudio.paInt16 # pyaudio.paInt16表示我们使用量化位数 16位来进行录音  tRATE = 44100 # 采样率 44.1k，每秒采样44100个点。  tWAVE_OUTPUT_FILENAME = /home/pi/chat/command.wav  tprint('请按住按钮开始录音...')  tGPIO.wait_for_edge(BUTT, GPIO.FALLING)  t# To use PyAudio, first instantiate PyAudio using pyaudio.PyAudio(), which sets up the portaudio system.  tp = pyaudio.PyAudio()  tstream = p.open(format = FORMAT,  t t t t tchannels = 1, t# cloud speecAPI只支持单声道  t t t t trate = RATE,  t t t t tinput = True,  t t t t tframes_per_buffer = CHUNK)  tprint(录音中...)  tframes = []  t# 按住按钮录音，放开时结束  twhile GPIO.input(BUTT) == 0:  t tdata = stream.read(CHUNK)  t tframes.append(data)  tprint(录音完成，输出文件： + WAVE_OUTPUT_FILENAME + '  ')  tstream.stop_stream()  tstream.close()  tp.terminate()  twf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')  twf.setnchannels(1)  twf.setsampwidth(p.get_sample_size(FORMAT)) t# Returns the size (in bytes) for the specified sample format.  twf.setframerate(RATE)  twf.writeframes(b''.join(frames))  twf.close()  t  treturn # 可以直接运行rec.py进行测试，同时保证该文件import时不会自动运行 if __name__ == '__main__':  trec_fun() 语音识别 参考： 使用Google云计算引擎实现科学上网 在Windows命令行、Linux终端使用代理 在Python中使用谷歌Cloud Speech API将语音转换为文字（另一种方案） 由于某些原因，笔者选择了使用谷歌Cloud Speech API进行语音识别。既然要用谷歌的服务，自然就涉及到了科学上网、代理、谷歌云平台的使用，如果不想这么折腾，完全可以用国内的讯飞、百度来实现。 另外，API KEY之类的字符串在这里删除了，还请麻烦修改代码加上你自己申请的API KEY。  文件 speech_api.py import json import urllib.request import base64 def wav_to_text():  tapi_url = https://speech.googleapis.com/v1beta1/speech:syncrecognize?key=替换成你的API密钥  tprint('语音文件编码中...')  taudio_file = open('/home/pi/chat/command.wav', 'rb')  taudio_b64str = (base64.b64encode(audio_file.read())).decode()  taudio_file.close()  tvoice = {  t config:  t {  t tlanguageCode: cmn-Hans-CN  t },  t audio:  t {  t tcontent: audio_b64str  t }  t}  tvoice = json.dumps(voice).encode('utf8')  tprint('编码完成。正在上传语音...')  treq = urllib.request.Request(api_url, data=voice, headers={'content-type': 'application/json'})  tresponse = urllib.request.urlopen(req)  tresponse_str = response.read().decode('utf8')  tresponse_dic = json.loads(response_str)  tif ('results' not in response_dic.keys()):  t tprint('您录制的文件似乎没有声音，请检查麦克风。')  t treturn  t  ttranscript = response_dic['results'][0]['alternatives'][0]['transcript']  tconfidence = response_dic['results'][0]['alternatives'][0]['confidence']  tresult_dic = {'text':transcript ,'confidence':confidence}  tprint('识别完成。以字典格式输出：')  tprint(result_dic)  t  treturn result_dic if __name__ == '__main__':  twav_to_text() 获取文字回答 参考：使用Python与图灵机器人聊天 这个获取回答的程序有些粗糙，只能获得普通的文字回答。实际上图灵机器人回复的内容中包括了文字、问题类型甚至情感等信息，还有很多修改的空间。  文件 turing.py import json import urllib.request def chat(question):  tapi_url = http://openapi.tuling123.com/openapi/api/v2  ttext_input = question['text']  treq = {  t tperception:  t t{  t t tinputText:  t t t{  t t t ttext: text_input  t t t},  t t tselfInfo:  t t t{  t t t tlocation:  t t t t{  t t t t tcity: 上海,  t t t t tprovince: 上海,  t t t t tstreet: 文汇路  t t t t}  t t t}  t t},  t tuserInfo:  t t{  t t tapiKey: 替换成你的APIKEY,  t t tuserId: 用户参数  t t}  t}  t# 将字典格式的req转为utf8编码的字符串  treq = json.dumps(req).encode('utf8')  t  tprint('  ' + '正在调用图灵机器人API...')  thttp_post = urllib.request.Request(api_url, data=req, headers={'content-type': 'application/json'})  tresponse = urllib.request.urlopen(http_post)  t  tprint('得到回答，输出为字典格式：')  tresponse_str = response.read().decode('utf8')  tresponse_dic = json.loads(response_str)  tintent_code = response_dic['intent']['code']  t  t# 返回网页类的输出方式  tif(intent_code == 10023):  t tresults_url = response_dic['results'][0]['values']['url']  t tresults_text = response_dic['results'][1]['values']['text']  t tanswer = {code: intent_code, text: results_text, url:results_url}  t tprint(answer)  t treturn(answer)  t# 一般的输出方式  telse:  t tresults_text = response_dic['results'][0]['values']['text']  t tanswer = {code: intent_code, text: results_text}  t tprint(answer)  t treturn(answer)  t if __name__ == '__main__':  teg_question = {'text': '今天是几号', 'confidence': 0.9}  tchat(eg_question) 读出回答（语音合成） 参考：在Python中使用科大讯飞Web API进行语音合成 笔者在使用讯飞Web API时，该服务才开放不到一周，难免以后该API会有所变动，如有问题建议查阅官方文档。  文件 tts.py import base64 import json import time import hashlib import urllib.request import urllib.parse import os def speak(text_content):  t# API请求地址、API KEY、APP ID等参数，提前填好备用  tapi_url = http://api.xfyun.cn/v1/service/v1/tts  tAPI_KEY = 替换成你的APIKEY  tAPP_ID = 替换成你的APPID  tAUE = lame  t# 构造输出音频配置参数  tParam = {  t tauf: audio/L16;rate=16000, t#音频采样率  t taue: AUE, t#音频编码，raw(生成wav)或lame(生成mp3)  t tvoice_name: xiaoyan,  t tspeed: 50, t#语速[0,100]  t tvolume: 10, t#音量[0,100]  t tpitch: 50, t#音高[0,100]  t tengine_type: aisound t#引擎类型。aisound（普通效果），intp65（中文），intp65_en（英文）  t}  t# 配置参数编码为base64字符串，过程：字典→明文字符串→utf8编码→base64(bytes)→base64字符串  tParam_str = json.dumps(Param) t#得到明文字符串  tParam_utf8 = Param_str.encode('utf8') t#得到utf8编码(bytes类型)  tParam_b64 = base64.b64encode(Param_utf8) t#得到base64编码(bytes类型)  tParam_b64str = Param_b64.decode('utf8') t#得到base64字符串  t# 构造HTTP请求的头部  ttime_now = str(int(time.time()))  tchecksum = (API_KEY + time_now + Param_b64str).encode('utf8')  tchecksum_md5 = hashlib.md5(checksum).hexdigest()  theader = {  t tX-Appid: APP_ID,  t tX-CurTime: time_now,  t tX-Param: Param_b64str,  t tX-CheckSum: checksum_md5  t}  t# 构造HTTP请求Body  tbody = {  t ttext: text_content  t}  tbody_urlencode = urllib.parse.urlencode(body)  tbody_utf8 = body_urlencode.encode('utf8')  t# 发送HTTP POST请求  tprint('  ' + 正在调用科大讯飞语音合成API...)  treq = urllib.request.Request(api_url, data=body_utf8, headers=header)  tresponse = urllib.request.urlopen(req)  t# 读取结果  tresponse_head = response.headers['Content-Type']  tif(response_head == audio/mpeg):  t tout_file = open('/home/pi/chat/answer.mp3', 'wb')  t tdata = response.read() # a `bytes` object  t tout_file.write(data)  t tout_file.close()  t tprint('得到结果，输出文件: /home/pi/chat/answer.mp3')  telse:  t tprint(response.read().decode('utf8'))  t# 播放音频  tprint(播放音频中...)  tprint(以下均为mplayer的输出内容  )  tos.system(mplayer -ao alsa:device=hw=1.0 /home/pi/chat/answer.mp3)  t  treturn  t if __name__ == '__main__':  teg_text_content = 苟利国家生死以，岂因祸福避趋之  tspeak(eg_text_content) 整合&amp;测试 现在，你的项目文件夹中应该有这些python代码文件： 接下来我们只需要将他们整合在一起运行。  文件 combine.py # 这些import进来的模块是同目录下的py文件 import rec t# rec.py负责录制wav音频 import speech_api t# speech_api.py负责wav转文字 import turing t# turing.py负责获得图灵机器人的文字回答 import tts t# tts.py负责读出回答 rec.rec_fun() t# 录制音频 recognize_result = speech_api.wav_to_text() t# 识别语音，返回值是字典格式，包含文字结果和信心 turing_answer = turing.chat(recognize_result) t# 得到图灵的回答，返回值仍是字典格式 tts.speak(turing_answer['text']) 如果一切顺利的话，实际运行效果如下： 树莓派_语音助手_youku 小结 语音助手这边的工作算是告一段落了，结果小结却不知道怎么写了。不管怎么说，很开心最后能得到实际的结果，做的过程中也有一些脑洞想要继续扩展，过段时间应该还会继续！ 做这个项目的过程中，项目外的收获或许比这个项目本身还要多。这段时间从很多博客、论坛得到了数不尽的帮助，国内的、国外的、中文的、英文的、日文的都有，深深地感受到了互联网共享精神的力量，这也是促使我开始写这些文章的原因。那么，最后还是说一句：感谢你阅读文章！ ",
      "url"      : "https://0qinghao.github.io/inforest/2018/03/26/PiDaXing/"
    } ,
  
    {
      "title"    : "常见内网网段",
      "category" : "network",
      "content": " 内网IP段有哪些 常见的内网IP段有： 10.0.0.0/8 10.0.0.0 - 10.255.255.255 172.16.0.0/12 172.16.0.0 - 172.31.255.255 192.168.0.0/16 192.168.0.0 - 192.168.255.255 以上三个网段分别属于A、B、C三类IP地址，来自 《RFC 1918》。 但是根据 《Reserved IP addresses - Wikipedia, the free encyclopedia》 及《RFC 6890 - Special-Purpose IP Address Registries》的描述， 还有很多其它的内网IP段（包括IPv6），以及其它用途的保留IP地址。 其它IPv4内网段罗列如下： 0.0.0.0/8 0.0.0.0 - 0.255.255.255 用于当前网络内的广播消息。 100.64.0.0/10 100.64.0.0 - 100.127.255.255 由运营商使用的私网IP段，随着IPv4地址池的耗光，会有更多用户被分配到这个网段。 127.0.0.0/8 127.0.0.0 - 127.255.255.255 本机回环地址。 169.254.0.0/16 169.254.0.0 - 169.254.255.255 获取不到IP地址时使用，通常因为从DHCP服务器获取不到IP。 255.255.255.255/32 255.255.255.255 本网段的广播地址。 ",
      "url"      : "https://0qinghao.github.io/inforest/2018/04/05/common-local-ip/"
    } ,
  
    {
      "title"    : "在Windows命令行、Linux终端使用代理",
      "category" : "proxy",
      "content": "在之前的博文中分享了如何使用Google云计算引擎搭建SS服务器，如何使用SS客户端，已经满足了科学上网的基本需求。这次将要总结在Windows的 CMD 窗口和Linux的 LX终端 中，让 wget curl 等命令使用代理需要进行的一些配置。 Windows命令行代理 假设你已经使用了SS客户端，本地socks5代理为127.0.0.1:1080 在CMD窗口输入如下指令设置代理： set http_proxy=socks5://127.0.0.1:1080 set https_proxy=socks5://127.0.0.1:1080 set ftp_proxy=socks5://127.0.0.1:1080 测试 curl https://www.facebook.com 能得到返回结果。 取消代理命令： set http_proxy= set https_proxy= set ftp_proxy= 设置代理后只对当前命令行窗口生效，重新打开CDM需要再次设置。 Linux LX终端代理 由于Linux下SS客户端仅代理socks5协议的流量（如果不是这个原因恳请指正）。所以想在LX终端使用代理，需要在SS的socks5流量前再接一个代理，允许http、https、ftp协议流量通过。 我们也假定本地socks5代理为127.0.0.1:1080 安装polipo Debian下直接使用apt命令安装： sudo apt update sudo apt install polipo 编辑配置文件： sudo nano /etc/polipo/config 配置内容如下 # This file only needs to list configuration variables that deviate # from the default values. See /usr/share/doc/polipo/examples/config.sample # and polipo -v for variables you can tweak and further information. logSyslog = true logFile = /var/log/polipo/polipo.log proxyAddress = 0.0.0.0 socksParentProxy = 127.0.0.1:1080 socksProxyType = socks5 chunkHighMark = 50331648 objectHighMark = 16384 dnsQueryIPv6 = no 按CTRL+X，Y保存退出。 重启polipo服务： sudo service polipo restart 启用代理 通过 service polipo status 命令，我们可以看到新的监听端口为8123。 因此，LX终端启用代理的命令为： export http_proxy=http://127.0.0.1:8123 export https_proxy=http://127.0.0.1:8123 export ftp_proxy=http://127.0.0.1:8123 同样，直接输入上述命令设置的代理也是临时的。一个比较实用的方法是在~/.bashrc文件中设置环境，之后就不需要再手动设置了。 sudo nano ~/.bashrc 在文件最后插入上述三条指令，保存。 测试 wget 指令： 小结 我对CMD/LX终端设置代理的出发点，是为了使用Google的一个API，设置后确实能够成功使用。另外似乎对 pip 等指令也有效果，安装python模块时的下载速度有比较明显的提升。不过说到底只是在总结如何使用别人做好的工具，很多原理还是没有明白，如果文中有何纰漏，恳请指正。 感谢你阅读文章！ ",
      "url"      : "https://0qinghao.github.io/inforest/2018/09/19/proxy-set-in-windows-and-linux/"
    } ,
  
    {
      "title"    : "译 - 使用iStyle格式化Verilog代码",
      "category" : "verilog",
      "content": " 原文：Verilogでコード整形 安装 iStyle可以从GitHub上clone、make自行编译出可执行文件，也可以直接下载已编译好的可执行文件。这里都给出来。 Github https://github.com/thomasrussellmurphy/istyle-verilog-formatter 可执行文件 https://github.com/HayasiKei/istyle-verilog-formatter/releases/tag/v1.21_x86_64 格式化选项 以下是一些格式化时常用的选项及效果示例。 待格式化代码 reg [3:0] cnt; always @(posedge clk or posedge rst) begin if(rst) begin cnt&lt;=4'h0; end else begin cnt&lt;=cnt+4'h1; end end –style ANSI style ./iStyle --style=ansi test.v reg [3:0] cnt; always @(posedge clk or posedge rst) begin  if(rst)  begin  cnt&lt;=4'h0;  end  else  begin  cnt&lt;=cnt+4'h1;  end end  Kernighan&amp;Ritchie style ./iStyle --style=kr test.v reg [3:0] cnt; always @(posedge clk or posedge rst) begin  if(rst) begin  cnt&lt;=4'h0;  end  else begin  cnt&lt;=cnt+4'h1;  end end  GNU style ./iStyle --style=gnu test.v reg [3:0] cnt; always @(posedge clk or posedge rst) begin  if(rst)  begin  cnt&lt;=4'h0;  end  else  begin  cnt&lt;=cnt+4'h1;  end end -s ./iStyle -s2 test.v 该选项指定缩进时的空格数量，-s2表示每次缩进使用2个空格（如果是-s4则表示每次用4个空格缩进）。 reg [3:0] cnt; always @(posedge clk or posedge rst) begin if(rst) begin  cnt&lt;=4'h0; end else begin  cnt&lt;=cnt+4'h1; end end -p -p选项指定在运算符两侧插入空格。 reg [3: 0] cnt; always @(posedge clk or posedge rst) begin  if (rst)  begin  cnt &lt;= 4'h0;  end else  begin  cnt &lt;= cnt + 4'h1;  end end -P -P选项指定在运算符和括号周围插入空格。 reg [ 3: 0 ] cnt; always @( posedge clk or posedge rst ) begin  if ( rst )  begin  cnt &lt;= 4'h0;  end else  begin  cnt &lt;= cnt + 4'h1;  end end 小结 虽然文中没有写，module声明的缩进感觉并不是很好。verilog有各种各样的代码风格，因此有一个强大的格式化程序是很有用的。 ",
      "url"      : "https://0qinghao.github.io/inforest/2019/08/09/translate-use-istyle-to-format-verilog-code/"
    } ,
  
    {
      "title"    : "Wiki Template",
      "category" : "cate1",
      "content": "Content here ",
      "url"      : "https://0qinghao.github.io/inforest/wiki/template/"
    } 
  
]

