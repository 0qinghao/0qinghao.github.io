

[
  
  
    {
      "title"    : "页面没有找到",
      "url"      : "https://0qinghao.github.io/inforest/404.html"
    } ,
  
  
  
    {
      "title"    : "About",
      "url"      : "https://0qinghao.github.io/inforest/about/"
    } ,
  
  
  
    {
      "title"    : "归档",
      "url"      : "https://0qinghao.github.io/inforest/archives/"
    } ,
  
  
  
    {
      "title"    : "Categories",
      "url"      : "https://0qinghao.github.io/inforest/categories/"
    } ,
  
  
  
  
  
    {
      "title"    : "Links",
      "url"      : "https://0qinghao.github.io/inforest/links/"
    } ,
  
  
  
    {
      "title"    : "Open Source Projects",
      "url"      : "https://0qinghao.github.io/inforest/open-source/"
    } ,
  
  
  
  
  
    {
      "title"    : "Wiki",
      "url"      : "https://0qinghao.github.io/inforest/wiki/"
    } ,
  
  
  
  
  
  
  
  
  
  
  
  
  
  

  
    {
      "title"    : "树莓派学习手记——修改软件源",
      "category" : "raspberrypi",
      "content": "国情，国情 在Raspbian/Ubuntu系统上，升级系统或安装软件只需要一条很简单的命令： sudo apt install 软件包名 t t# 安装软件 sudo apt upgrade t t# 更新软件 然而在天朝的网络下，很难顺利地完成下载过程。但好在有许多高校/机构提供了及时更新的镜像网站，我们可以通过修改配置文件解决下载难的问题。 很多同学查找解决方法后，或许能解决一部分问题，但仍会遇到连接超时的问题。究其原因，大致有两点：  树莓派的软件源配置有两处，而大部分教程只指出了一处；   没有区分系统版本（Codename），Codename目前分为jessie / wheezy / squeeze / stretch，大部分教程仍使用的是jessie或wheezy，而笔者安装的系统却是stretch。 ​ 配置文件在哪 /etc/apt/sources.list /etc/apt/sources.list.d/raspi.list 很多教程只指出了第一处，如果没有修改第二个配置文件，更新系统时很容易出现连接超时的问题。 在修改配置文件之前，可以选择先备份一下原文件，但这个配置文件也不太重要，不想麻烦也可跳过。 sudo cp /etc/apt/sources.list /etc/apt/sources.bak sudo cp /etc/apt/sources.list.d/raspi.list /etc/apt/sources.list.d/raspi.bak ​ 我的Codename是什么 我们来确定自己树莓派安装的系统Codename是什么： lsb_release -a 运行这条指令之后，可以很清楚的看到Codename Codename: stretch ​ 修改配置文件 国内有许多高校提供了树莓派的软件源镜像。可以在这个网页查看所有的镜像网站：http://www.raspbian.org/RaspbianMirrors 笔者选择了中科大提供的镜像，也是大家公认的比较稳定的镜像之一。 sudo nano /etc/apt/sources.list 将该文件的内容替换为： deb http://mirrors.ustc.edu.cn/raspbian/raspbian/ stretch main contrib non-free deb-src http://mirrors.ustc.edu.cn/raspbian/raspbian/ stretch main contrib non-free 按CTRL+X 关闭文件，键入Y（保存修改）回车。 修改第二个配置文件： sudo nano /etc/apt/sources.list.d/raspi.list 类似地，内容替换为： deb http://mirrors.ustc.edu.cn/archive.raspberrypi.org/ stretch main ui deb-src http://mirrors.ustc.edu.cn/archive.raspberrypi.org/ stretch main ui 相信细心的同学已经注意到了，修改的文件内容网址后紧接着一项”stretch”。如果你手中的树莓派安装的系统Codename并不是stretch，还请进行相应修改。 最后，刷新软件列表： sudo apt update 修改完成了！赶紧去体验一下高速更新系统/升级软件的快感吧。感谢你阅读文章！ ",
      "url"      : "https://0qinghao.github.io/inforest/2018/01/06/RPi-apt-source/"
    } ,
  
    {
      "title"    : "申请和使用Google云计算引擎配置SS Server",
      "category" : "google cloud",
      "content": "Google Cloud Platform的新用户可以获得$300赠金的一年使用权，使用这笔不菲的赠金，我们可以构建应用程序、搭建网站、存储数据、体验各种强大的API。这次，我总结了使用Google云计算引擎搭建SS服务器，实现科学上网的过程，也算作为墙内使用谷歌云平台的第一步。 科学上网的基本原理 我们只需要一个能够访问墙外目标地址的代理服务器。本地设备向服务器发送访问目标地址的请求，服务器收到请求后访问目标并将结果回传给本地设备。 我们是使用Shadowsocks（简称SS，中文名影梭）来配置服务器的，所以一般把这个服务器称为SS服务器。谷歌云平台提供的位于国外的云计算引擎可以用来搭建SS服务器。 申请试用谷歌云平台 *重要：你需要一张外币信用卡（VISA/MasterCard/JCB） 首先，翻墙。突然有种鸡生蛋，蛋生鸡的矛盾，不过我相信你能找到一个免费试用的VPN。 登录谷歌云平台，点击右上角的申请试用后进入申请界面。地区可以选择中国，不影响后续的申请。 账号类型选择“个人”，填写名称地址电话。 付款方式填写你的外币信用卡（单币银联卡无效）。提交后信用卡会扣除1美金进行验证，验证完成即退回。 创建计算引擎 进入控制台，首先要求创建一个项目，尽量使用简单易记的项目名。 项目创建完成后，点击控制台左上角的 ☰ 打开导航栏，找到 Compute Engine → VM实例 ，点击 创建 开始创建一个计算引擎。 区域 有3个比较好的选择： asia-east1：位于台湾 asia-southeast1：位于新加坡 asia-northeast1：位于东京 从国内ping延迟都在100ms左右，它们的流量费用和硬件费用有细微的差别，在意的朋友可以在这里查询。 机器类型 可以选择最小的微型（1个共享vCPU，0.6GB内存）以节省硬件费用，单作为SS服务器该配置已经足够。 其他设置可以保持默认。点击 创建 。 配置SS服务器 创建完成后可以看到分配给实例的 外部IP ，请牢记。 点击云引擎后面的 SSH ，远程连接该主机，进行配置。 这里使用秋水逸冰大大的SS服务器配置脚本。 依次输入下面三条指令： wget --no-check-certificate -O shadowsocks.sh https://raw.githubusercontent.com/wjk199511140034/ss-onekeyinstall/master/shadowsocks.sh sudo chmod +x shadowsocks.sh sudo ./shadowsocks.sh 2&gt;&amp;1 | tee shadowsocks.log 第三条指令运行后即进入配置过程，需要根据提示输入几项信息。 Please input password for shadowsocks-libev：输入 密码 ，请牢记 Please enter a port for shadowsocks-libev：输入SS 服务器端口号 ，请牢记 Which cipher you’d select：选择一种 加密方式 ，请牢记 按任意键开始执行脚本，等待脚本运行完毕。 创建防火墙规则 点击控制台左上角的 ☰ 打开导航栏，找到 VPC网络 → 防火墙规则 ，点击 创建防火墙规则 创建如下2个规则。  入站规则 流量方向 ：入站 目标 ：网络中的所有实例 来源 IP 地址范围 ：0.0.0.0/0 协议和端口 ：全部允许 其他部分可以保持默认，这条规则表示允许所有ip/端口的所有协议入站。  出站规则 流量方向 ：出站 目标 ：网络中的所有实例 来源 IP 地址范围 ：0.0.0.0/0 协议和端口 ：全部允许 其他部分可以保持默认，这条规则表示允许所有协议出站到所有ip/端口。 至此，SS服务器部署完毕。你可以关闭你不稳定的试用版VPN，准备开始正确地科学上网了。 使用SS客户端 这里仅以Windows客户端为例，Android端很相似。Debian平台使用SS客户端则需要进行一些配置，将另外做一次总结。 可以在GitHub下载到Windows平台的SS客户端。 如果你无法打开GitHub，可以点击这里，前往微云下载，但不保证是最新版本。 请将可执行程序放置在合适的文件夹内，运行后会在程序同一目录下产生配置文件，如果随便放置容易显得杂乱。 第一次打开SS客户端会主动要求编辑服务器。填入你的 外部IP 密码 服务器端口号 加密方式 ，其他设置可以保持默认。 最后，右击任务栏的小图标，勾选 启用系统代理 。系统代理模式选择 PAC模式 ，这样SS会自动使用代理访问墙外站点，不需要另外安装浏览器的代理插件。 参考资料 Debian下shadowsocks-libev一键安装脚本 Shadowsocks Troubleshooting Shadowsocks原理和搭建 Google Cloud服务免费申请试用以及使用教程 感谢你阅读文章！ ",
      "url"      : "https://0qinghao.github.io/inforest/2018/02/27/google-cloud-engine-ss-server/"
    } ,
  
    {
      "title"    : "SS Debian服务器端配置指令",
      "category" : "shadowsocks linux",
      "content": "笔记 wget --no-check-certificate -O shadowsocks-libev-debian.sh https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks-libev-debian.sh sudo chmod +x shadowsocks-libev-debian.sh sudo ./shadowsocks-libev-debian.sh 2&gt;&amp;1 | tee shadowsocks-libev-debian.log ",
      "url"      : "https://0qinghao.github.io/inforest/2018/02/28/debian-ss-server-config-command/"
    } ,
  
    {
      "title"    : "Shadowsocks客户端在不同系统下的使用方法",
      "category" : "shadowsocks",
      "content": "当我们配置好Shadowsocks服务器端，或是购买了SS账号后，就可以使用客户端开始科学上网了。下面分别介绍在Windows、安卓、Linux(Debian)系统下SS客户端的使用方法。 Windows系统下SS客户端的使用方法 Windows系统下的SS客户端使用起来最为方便。客户端自带了系统全局代理的功能，甚至可以省去配置浏览器插件的操作。 下载客户端  点击这里，跳转到GitHub下载 如果你无法打开GitHub，可以点击这里，前往微云下载，但不保证是最新版本 将压缩包内的可执行程序解压，放置在合适的文件夹内，运行后会在程序同一目录下产生配置文件，如果随便放置容易显得杂乱。 配置客户端 第一次打开SS客户端会主动要求编辑服务器。填入你的 服务器地址 密码 服务器端口号 加密方式 ，其他设置可以保持默认。 最后，右击任务栏的小图标，勾选 启用系统代理 。系统代理模式选择 PAC模式 ，这样SS会自动使用代理访问墙外站点，不需要另外安装浏览器的代理插件。 安卓系统下SS客户端的使用方法 安卓系统下的SS客户端也很完善，配置方便，甚至还可以指定仅部分APP使用代理。 下载客户端  如果你能使用Google Play商店，直接搜索安装Shadowsocks 你也可以点击这里，前往微云下载，但不保证是最新版本 配置客户端 点击右上角的 + 选择 手动设置 ，填入你的 服务器地址 密码 服务器端口号 加密方式 ，其他设置可以保持默认。 还可以在配置中开启 分应用VPN 功能，来指定仅部分APP的流量进行代理；或者再打开 绕行模式 来指定部分APP的流量绕过代理。 Debian下SS客户端的使用方法 Linux下使用SS客户端要麻烦一些，一方面Linux下SS不带全局代理，需要搭配浏览器插件使用；另一方面笔者在使用中有遇到bug，不知在你阅读这篇文章时是否已经修复，总之还是会记录在下文中以供参考。 另外，这部分介绍的是配合Chrome插件实现浏览器翻墙的方法。关于如何在LX终端让 wget curl 等命令使用代理，将在另一篇文章中再做总结。 安装客户端 sudo apt update sudo apt install shadowsocks 运行sslocal 不带任何参数运行 sslocal 可以查看帮助。 运行SS客户端一般有两种方法。你可以参考帮助，将必要的参数填入，用一条较长的指令来运行： sudo sslocal -s 服务器地址 -p 服务器端口 -k 密码 -m 加密方式 -d start 显然上面这种方式效率太低。另一种方式就是将各项参数保存为json文件，运行时指定配置文件即可。 假设我们的配置文件是 /etc/ss.json ，其内容为： { server:服务器地址, server_port:服务器端口, local_address:127.0.0.1, local_port:1080, password:密码, timeout:600, method:加密方式, fast_open:false } 将你的 服务器地址 密码 服务器端口号 加密方式 替换到上述文件。（有双引号的请保留双引号，不要删除） 接下来，每次需要运行SS客户端时，我们只需要输入一条简短的指令： sudo sslocal -c /etc/ss.json -d start *运行sslocal时遇到的bug  解决方案来自Kali2.0 update到最新版本后安装shadowsocks服务报错问题 笔者在运行sslocal命令时遇到了形如 INFO loading libcrypto from libcrypto.so.1.1 的报错。后在上述文章中找到解决方案。 打开文件openssl.py，请参照错误提示确定是否与下述文件路径相同： sudo nano /usr/local/lib/python2.7/dist-packages/shadowsocks/crypto/openssl.py 使用快捷键CTRL+_选择跳转到第52行 将 libcrypto. EVP_CIPHER_CTX_ cleanup .argtypes = (c_void_p, ) 修改为 libcrypto. EVP_CIPHER_CTX_ reset .argtypes = (c_void_p, ) 同样地，跳转到第111行 将 libcrypto. EVP_CIPHER_CTX_ cleanup (self.ctx) 修改为 libcrypto. EVP_CIPHER_CTX_ reset (self._ctx) 按CTRL+X，Y保存退出。重新执行sslocal指令运行正常。 安装Chrome插件 由于Linux下的SS客户端不带全局代理功能，需要配合浏览器插件使用。这里只介绍Chrome插件的安装方法，火狐大体上类似。 如果你能够使用Chrome应用商店，搜索SwitchyOmega安装即可。你也可以点击这里通过微云下载crx文件，将其拖动到Chrome扩展程序页面完成安装。 点击选项，如下图配置SwitchyOmega。 代理协议 选择 SOCKS5 ；如果你在ss.json配置文件中修改过 local_port 参数，则这里 代理端口 必须与其一致，否则保持默认值1080即可。 最后，保存配置，点击SwitchyOmega图标切换到刚才配置好的情景模式。 结语 由于手上设备有限，没办法总结所有系统下的SS客户端使用方法。例如iOS系统下似乎是使用Big Boss源搜索ShadowSocks应用，但没法亲自尝试。有兴趣的朋友建议前往官网https://shadowsocks.org/（.com那个是出售SS服务的）进一步了解。 感谢你阅读文章！ ",
      "url"      : "https://0qinghao.github.io/inforest/2018/03/06/shadowsocks-clients/"
    } ,
  
    {
      "title"    : "在Python中使用谷歌Cloud Speech API将语音转换为文字（另一种方案）",
      "category" : "google cloud",
      "content": "在之前发布的使用谷歌Cloud Speech API将语音转换为文字一文中，我们实现了在控制台使用curl发送post请求，得到语音转文字的结果；而在Python中使用谷歌Cloud Speech API将语音转换为文字一文中，我们实现了安装Cloud Speech API客户端库，通过调用库函数得到语音转文字的结果。 如果你尝试过这两种方法，就会发现其实后者得到结果需要的时间要长一些（笔者使用这两种方法得到结果的耗时分别大约是5秒、7秒）。那么，有没有办法在python中像第一种方法那样，使用curl命令发送post请求呢。当然是可行的，所以今天我们将介绍在Python中使用Cloud Speech API将语音转换为文字的另一种方案，另外这次我们将把音频文件编码为base64嵌入到json请求文件中，省去了上传声音文件到Cloud Storage的步骤。 相关说明之类的在上面两篇文章里已经写了很多，这边就直接贴代码。 *使用python3 import json import urllib.request import base64 # ① api_url = https://speech.googleapis.com/v1beta1/speech:syncrecognize?key=你的API密钥 audio_file = open('/home/pi/chat/test-speech/output.wav', 'rb') audio_b64 = base64.b64encode(audio_file.read()) audio_b64str = audio_b64.decode() t# ② # print(type(audio_b64)) # print(type(audio_b64str)) audio_file.close() # ③ voice = { config: {  #encoding: WAV,  languageCode: cmn-Hans-CN }, audio: {  content: audio_b64str } } # 将字典格式的voice编码为utf8 voice = json.dumps(voice).encode('utf8') req = urllib.request.Request(api_url, data=voice, headers={'content-type': 'application/json'}) response = urllib.request.urlopen(req) response_str = response.read().decode('utf8') # ④ # print(response_str) response_dic = json.loads(response_str) transcript = response_dic['results'][0]['alternatives'][0]['transcript'] confidence = response_dic['results'][0]['alternatives'][0]['confidence'] print(transcript) print(confidence) 几点说明： 注释 ① ：请求API的链接，请替换 你的API密钥 。如果你有疑问，或许可以参考 创建API密钥 - 使用谷歌Cloud Speech API将语音转换为文字 。 audio_file 路径替换为你的本地声音文件路径。 注释 ② ：这次上传音频的方式是，将声音文件编码为base64，把对应的整个字符串放进json请求中。如果你执行 print(type(audio_b64)) 就会发现编码后的audio_b64是 bytes 类型，所以还需要做一次decode()，转成字符串。 注释 ③ ：先以字典格式保存json请求内容，代表声音文件的字符串就在这里放入。 注释 ④ ：API返回的结果保存在 response_str ，如果你直接运行 print(response_str) 就会发现这个字符串可以看做一个有很多“层”的字典，要提取出识别结果，需要搞清楚这个字典到底是怎么组成的： 第 1 层：花括号{}说明字符串 response_str 在执行 json.loads 后变成一个”字典”。得到”字典” response_dic 。 第 2 层：字典中只有一组键-值， response_dic['results'] 取出唯一的键”results”对应的值。观察这个值，发现中括号[]，说明这个值的类型是”列表“。 第 3 层：观察列表 response_dic['results'] ，发现列表中只有一项数据，但这项数据又是”字典”类型。将其取出，得到”字典” response_dic['results'][0] 。 第 4 层：字典中又是只有一组键-值， response_dic['results'][0]['alternatives'] 取出唯一的键”alternatives”对应的值。观察这个值，[]说明我们得到的结果又是一个”列表”。 第 5 层：观察列表 response_dic['results'][0]['alternatives'] ，列表中只有一项数据， response_dic['results'][0]['alternatives'][0] 再将这唯一一项数据取出，发现得到的是一个”字典”，而这次得到的字典中有两组键-值，分别取出就是我们要的结果和置信度了。 transcript = response_dic['results'][0]['alternatives'][0]['transcript'] confidence = response_dic['results'][0]['alternatives'][0]['confidence']  小结： 今天介绍的这种方案，获取结果需要的时间比用API客户端库要快一些，另外应用了把本地语音编码后放入json请求的方式，也能方便后期和录音程序结合在一起使用。但稍有一点缺点是API密钥直接暴露在代码中，对实际应用可能会有一些影响。 下一步的目标是和录音功能结合起来，实现自动识别当前录制的语音。 感谢你阅读文章！ ",
      "url"      : "https://0qinghao.github.io/inforest/2018/03/08/google-cloud-speech-api-voice2text-python-another-way/"
    } ,
  
    {
      "title"    : "在Python中使用谷歌Cloud Speech API将语音转换为文字",
      "category" : "google cloud",
      "content": "之前我们总结了使用谷歌Cloud Speech API将语音转换为文字的基本流程，然而那只是在命令行中使用curl实现的。这次我们将总结在Python中使用Cloud Speech API的方法。 配置Python开发环境 笔者使用的是树莓派（Debian）进行试验的，其他平台的配置方法可以在这里查找。 安装Python 大多数Linux发行版都包含Python。对于Debian和Ubuntu，运行以下指令确保Python版本是最新的： sudo apt update sudo apt install python python-dev python3 python3-dev python-pip python3-pip 安装和使用virtualenv 尽管这一步不是必须的，但强烈建议你使用virtualenv。virtualenv是一种创建独立Python环境的工具，可以将每个项目的依赖关系隔离开来。在虚拟环境下，你可以不必顾虑python2和python3的冲突；另外一个优势是可以直接将你的项目文件夹复制到其他机器上，文件夹内就包含了项目所依赖的软件包。 sudo apt install python-virtualenv 安装完成后，就可以在你的项目文件夹中创建一个虚拟环境。 cd 项目文件夹 virtualenv --python python3 env 使用 --python 标志来告诉virtualenv要使用哪个Python版本，这次试验将全程以python3环境进行。执行后会在 项目文件夹 内创建一个 env 文件夹。 创建完成后，你需要“激活”virtualenv。激活virtualenv会告诉你的shell为Python使用virtualenv的路径。 source env/bin/activate 看到激活虚拟环境后，你就可以放心地安装软件包，并确信它们不会影响其他项目。 如果你想停止使用virtualenv并返回到全局Python环境，你可以关闭它： deactivate 配置Cloud Speech API客户端库 我们假定你已经有合适的代理，能够使用谷歌服务，并且已经开始使用Google云平台。如果你有疑问，或许可以参考这篇文章。 安装客户端库 如果你安装了virtualenv，请确保激活了虚拟环境。 pip install --upgrade google-cloud-speech 值得一提的是，笔者使用的树莓派在安装进行到 Running setup.py bdist_wheel for grpcio ... 时停留了非常久（10分钟以上），这属于正常现象，树莓派编译进行得很慢，需要耐心等待。 设置验证 登录谷歌云平台控制台，前往创建服务账号密钥界面。 从 服务帐户 下拉列表中选择 新建服务帐户 。输入合适的 服务帐号名称 ， 角色 选择 Project → 所有者 。 密钥类型 选择 JSON 。 点击 创建 后，会开始下载包含密钥的JSON文件，请妥善保存 。 最后，将环境变量 GOOGLE_APPLICATION_CREDENTIALS 设置为含密钥的JSON文件的文件路径，例如： export GOOGLE_APPLICATION_CREDENTIALS=/home/pi/speech/speech-account.json 请将 /home/pi/speech/speech-account.json 替换为你的json文件路径。 当然，直接输入上述命令设置的环境变量是临时的。一个比较实用的方法是在 ~/.bashrc 文件中设置环境，之后就不需要再手动设置了。 sudo nano ~/.bashrc 在文件末尾插入上述 export 命令，保存。 使用客户端库 下例给出了使用客户端库的方法。 import io import os # Imports the Google Cloud client library from google.cloud import speech from google.cloud.speech import enums from google.cloud.speech import types # Instantiates a client client = speech.SpeechClient() # The name of the audio file to transcribe file_name = os.path.join(  os.path.dirname(__file__),  'voice.wav') # Loads the audio into memory with io.open(file_name, 'rb') as audio_file:  content = audio_file.read()  audio = types.RecognitionAudio(content=content) config = types.RecognitionConfig(  encoding=enums.RecognitionConfig.AudioEncoding.LINEAR16,  sample_rate_hertz=16000,  language_code='cmn-Hans-CN') # Detects speech in the audio file response = client.recognize(config, audio) for result in response.results:  print('Transcript: {}'.format(result.alternatives[0].transcript))  print('Confidence: {}'.format(result.alternatives[0].confidence)) 几点说明： file_name 给出了声音文件的路径。其中 os.path.dirname(__file__) 表示py代码所在文件夹的路径。故上例中声音文件是py代码相同目录下的 voice.wav 。 config 给出了声音文件的编码信息，Cloud Speech API并不支持任意格式的声音文件，详细要求参见：AudioEncoding - Google Cloud Speech API 。 language_code='cmn-Hans-CN' 表示识别语言为中文普通话。常用的还有American English ( en-US )、British English ( en-GB )、日本語( ja-JP )、廣東話( yue-Hant-HK )。更多语言支持可以在Language Support - Google Cloud Speech API查询。 运行结果： “Confidence”是置信度，越接近1准确性越高。 小结 至此，Cloud Speech API的使用总结就告一段落了，希望能对你有所帮助。这篇总结是参照着Google Cloud Speech API文档写下的，如果有何纰漏恳请指出。 感谢你阅读文章！ ",
      "url"      : "https://0qinghao.github.io/inforest/2018/03/08/google-cloud-speech-api-voice2text-python/"
    } ,
  
    {
      "title"    : "使用谷歌Cloud Speech API将语音转换为文字",
      "category" : "google cloud",
      "content": "Google Cloud Speech API是由谷歌云平台提供的，利用机器学习技术将语音转换为文字的服务。这个API能识别超过80种语言和语言变体，包括中文、日语、英语甚至广东话。这次，我总结了使用Google Cloud Speech API的基本流程。 花5秒钟试用Cloud Speech API吧 在Cloud Speech API概览页，我们可以体验将语音转换为文字的效果。只需要选择一种语言即可开始使用，甚至不需要登录谷歌账号。（加载出来需要一些时间） 在项目中添加API 使用Cloud Speech API需要登录谷歌云平台并申请免费试用，申请试用谷歌云平台的流程可以参考这篇文章 。 我们假定你能够使用谷歌云平台，并且已经创建了一个项目，下面介绍如何把Cloud Speech API添加到项目中。 点击控制台左上角的 ☰ 打开导航栏，找到 API和服务 → 库 。 在搜索框中键入 Speech 即可找到 Cloud Speech API 。 打开API页面，点击 启用 。 创建API密钥 回到之前的页面，选择 凭据 → 创建凭据 → API密钥 。 马上 API密钥 就创建好了，虽然随时都能在这个页面查询，但为了方便起见，将其记录下来备用吧，很快就要用到它。 限制密钥 选项默认情况下应该是“无”，这次只是试着使用API，保持默认“无”即可。 准备声音文件 虽然有些麻烦，但是接下来我们要准备声音文件。Cloud Speech API没办法直接识别mp3、mp4中的声音，我们需要准备FLAC、WAV格式的音频。而且仅支持单声道音频，所以一般都需要转码之类的工作。 详细的声音文件要求参见：AudioEncoding - Google Cloud Speech API 基于上述情况，我读了下面这段文稿，并制成了FLAC格式（单声道）的声音文件。是用手机麦克风进行录音的，质量一般(´・ω・｀) 是否可以正确识别呢？ voice.flac  寄蜉蝣于天地，渺沧海之一粟。哀吾生之须臾，羡长江之无穷。挟飞仙以遨游，抱明月而长终。 《赤壁赋》 将声音文件上传到Cloud Storage 如果要使用Cloud Speech API识别本地声音文件，必须将音频文件编码为base64，然后嵌入到稍后将创建的json请求文件中，这虽然可行但并不方便。如果你想使用这种方法，请参考：Embedding Base64 encoded audio - Google Cloud Speech API 我们将使用另一种方案，将声音文件上传到Google Cloud Storage。 点击控制台左上角的 ☰ 打开导航栏，找到 存储 → 浏览器 。 点击 创建存储分区 。 输入合适的 存储分区名称，后文将要用到。默认存储类别选择”Multi-Regional”，Multi-Regional位置选择”亚洲”。点击 创建 。 点击 上传文件 ，上传声音文件，勾选 公开链接 。（该音频将能被任何人访问，请注意）  2018年10月18日更新： 刚看了一下，页面有所改变，暂时没找到公开单个音频文件的方法。 你可以这样做，把整个存储分区公开： 导航栏→存储→浏览器→存储分区最后有个选项，点开来→修改存储分区权限→“添加成员”填“allUsers”，“角色”选“存储对象查看者”→添加 注意：这样该分区内所有内容都可能被任何人访问到 另外，请记住上传文件的 文件名 ，后文将用到。 将语音转换为文字 终于，可以使用Cloud Speech API将语音转换为文字了。 首先，我们新建一个json格式的请求文件（request.json）。文件名无特殊要求。 { config: {  encoding:FLAC,  languageCode:cmn-Hans-CN }, audio: {  uri:gs://存储分区名称/文件名 } } 注意3个地方： cmn-Hans-CN ：表示识别语言为中文普通话。常用的还有American English ( en-US )、British English ( en-GB )、日本語( ja-JP )、廣東話( yue-Hant-HK )。更多语言支持可以在Language Support - Google Cloud Speech API查询。 存储分区名称 ：刚才是否有记录下来呢？如果没有记住可以点击控制台左上角的 ☰ 打开导航栏，找到 存储 → 浏览器 查看。 文件名 ：存储在Cloud Storage中的音频文件名，可以在存储分区中查看。 最后，我们使用curl命令（Windows平台需另外安装）向Cloud Speech API发出请求。 cd到json请求文件所在目录。  curl -H “Content-Type: application/json” -d @request.json “https://speech.googleapis.com/v1/speech:recognize?key=API密钥” 注意2个加粗处： request.json ：json请求文件的文件名。 API密钥 ：替换为你记录下来的API密钥。如果没有记下来，可以点击控制台左上角的 ☰ 打开导航栏，找到 API和服务 → 凭据 查看。 得到结果： 可以看到返回结果也是json格式的数据。”confidence”是置信度，越接近1准确性越高。 小结 第一次尝试语音识别服务，得到结果的时候很开心。或许有人会惊讶上例语音识别的准确性，但正如文章开头所说“Cloud Speech API是利用机器学习技术将语音转换为文字的服务”，像上例中这样的俗语、名著甚至是歌词，准确率都出奇地高。如果你录制一段日常语音交给Cloud Speech API识别，结果就不那么满意了。 最后，这次只是使用curl命令在LX终端获得了识别结果，下次将会总结如何在编程语言中使用Cloud Speech API。 感谢你阅读文章！ ",
      "url"      : "https://0qinghao.github.io/inforest/2018/03/08/google-cloud-speech-api-voice2text/"
    } ,
  
    {
      "title"    : "修改pip源",
      "category" : "python",
      "content": "linux下，修改 ~/.pip/pip.conf (没有就创建一个)，内容如下： [global] index-url = https://pypi.tuna.tsinghua.edu.cn/simple windows下，直接在user目录中创建一个pip目录，其中新建文件pip.ini，如：C:/Users/xx/pip/pip.ini，内容如下 [global] index-url = https://pypi.tuna.tsinghua.edu.cn/simple ",
      "url"      : "https://0qinghao.github.io/inforest/2018/03/16/config-pip-source/"
    } ,
  
    {
      "title"    : "在virtualenv中安装PyAudio",
      "category" : "python",
      "content": "如果你想在一个virtualenv中安装PyAudio，请安装APT中的PortAudio开发头文件，然后安装PyAudio： sudo apt-get install portaudio19-dev pip install --allow-unverified=pyaudio pyaudio ",
      "url"      : "https://0qinghao.github.io/inforest/2018/03/16/python-virtualenv-pyaudio/"
    } ,
  
    {
      "title"    : "SSH密钥登录配置流程",
      "category" : "linux",
      "content": "笔记 ssh-keygen cd .ssh cat id_rsa.pub &gt;&gt; authorized_keys chmod 600 authorized_keys chmod 700 ~/.ssh # sudo systemctl enable ssh sudo systemctl start ssh # 有问题检查配置文件 sudo vim /etc/ssh/sshd_config 将私钥文件 id_rsa 下载到客户端机器上。 如果使用putty的话，打开 PuTTYGen，Import私钥文件。 载入成功后，PuTTYGen 会显示密钥相关的信息。在 Key comment 中键入对密钥的说明信息，然后单击 Save private key 按钮即可将私钥文件存放为 PuTTY 能使用的格式。 今后，使用 PuTTY 登录时，可以在左侧的 Connection -&gt; SSH -&gt; Auth 中的 Private key file for authentication: 处选择私钥文件，即可登录了。 ",
      "url"      : "https://0qinghao.github.io/inforest/2018/03/17/linux-ssh-key-config/"
    } ,
  
    {
      "title"    : "树莓派学习手记——使用Python录音",
      "category" : "raspberrypi",
      "content": "有的时候我们想让树莓派能够录音，以实现语音控制等功能。所以今天我们总结一下用在树莓派上使用Python录音的过程。 准备硬件 树莓派上自带的3.5mm接口只能作为语音输出口，不能接麦克风。所以我们需要另外购买USB声卡，某宝上5元左右就能买到，当然你还需要一个麦克风。总费用应该在20元以内。 检查硬件是否正常 使用arecord -l可以列出所有录音设备，一般输出如下： arecord -l List of CAPTURE Hardware Devices card 1: Device [USB Audio Device], device 0: USB Audio [USB Audio] Subdevices: 1/1 Subdevice #0: subdevice #0 同样地，aplay -l可以列出所有播放设备，输出中也能找到形如 Device [USB Audio Device] 的设备。 我们可以直接在命令行执行Linux自带的录音/播放命令，测试硬件是否正常： arecord -D hw:1,0 -t wav -c 1 -r 44100 -f S16_LE test.wav aplay -D hw:0,0 test.wav arecord 是录音命令，其中 hw:1,0 表示 card 1: Device [USB Audio Device], device 0: USB Audio [USB Audio] 的 card 1 , device 0 ，如果你的USB声卡录音设备不是 card 1 , device 0 ，还请进行相应修改。另外，录音过程需要手动按CTRL + C结束。 aplay 是播放命令，其中 hw:0,0 表示树莓派板载音频接口，如果你把耳机插在USB声卡接口，还请进行相应修改，如改成 hw:1,0 。 如果你发现录制的音频内没有声音，只有细微的杂音，但 arecord -l 和 aplay -l 列出的设备中确实有USB声卡。那么你可以尝试着把麦克风接口拔出来一些，只插进去2/3，或许能够解决你的问题。笔者不是很明白其中的缘由，如果你有什么想法恳请留言告知。 安装pyaudio 在Python中执行录音命令需要pyaudio模块，直接用pip命令安装： pip install pyaudio 如果你使用pip命令下载速度很慢，或许修改pip源可以帮到你。 如果你使用了virtualenv，一般会发现pyaudio安装失败。这种情况下你需要安装APT中的PortAudio开发头文件，然后安装PyAudio： sudo apt-get install portaudio19-dev pip install pyaudio 使用Python录音 该例程修改自官方主页例程PyAudio。 import pyaudio import wave import os import sys CHUNK = 512 FORMAT = pyaudio.paInt16 CHANNELS = 1 RATE = 44100 RECORD_SECONDS = 5 WAVE_OUTPUT_FILENAME = output.wav p = pyaudio.PyAudio() stream = p.open(format=FORMAT,    channels=CHANNELS,    rate=RATE,    input=True,    frames_per_buffer=CHUNK) print(recording...) frames = [] for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):  data = stream.read(CHUNK)  frames.append(data) print(done) stream.stop_stream() stream.close() p.terminate() wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb') wf.setnchannels(CHANNELS) wf.setsampwidth(p.get_sample_size(FORMAT)) wf.setframerate(RATE) wf.writeframes(b''.join(frames)) wf.close() 执行后会录制一段5秒的音频，输出为同目录下的output.wav文件。 python3 rec.py * 隐藏错误消息 一般情况下，在树莓派上执行上述Python代码后，你会看到非常多的ALSA报错和JACK报错：  ALSA lib confmisc.c:1281:(snd_func_refer) Unable to find definition ‘cards.bcm2835.pcm.front.0: CARD=0’ …… …… connect(2) call to /tmp/jack-1000/default/jack_0 failed (err=No such file or directory) attempt to connect to server failed 但你会发现其实能够正常地录音。如果你不想看到这些错误消息，可以在代码中加入下述命令隐藏错误： os.close(sys.stderr.fileno()) 小结 使用Python录音很简单，你还可以在GPIO口上接入一个按钮，修改例程，实现按下按钮自动开始录音的功能。下一步的目标是把Python录音和Cloud Speech API语音识别结合起来。 感谢你阅读文章！ ",
      "url"      : "https://0qinghao.github.io/inforest/2018/03/20/RPi-recorder-python/"
    } ,
  
    {
      "title"    : "python json.dumps中的ensure_ascii参数引起的中文编码问题",
      "category" : "python",
      "content": "json.dumps序列化时对中文默认使用的ascii编码，想输出真正的中文需要指定ensure_ascii=False。 result_str = json.dumps(result, ensure_ascii = False) ",
      "url"      : "https://0qinghao.github.io/inforest/2018/03/22/python-json-dumps-ensure-ascii-chinese-character-code/"
    } ,
  
    {
      "title"    : "使用Python与图灵机器人聊天",
      "category" : "python",
      "content": "图灵机器人对中文的识别准确率高达90%，是目前中文语境下智能度最高的机器人。有很多在Python中使用图灵机器人API的博客，但都是1.0版本。所以今天简单地总结一下在Python中使用图灵机器人API v2.0的方法。 获取API KEY 首先，前往图灵机器人官方网站 http://www.tuling123.com/ 注册账号。 登录后点击 创建机器人 ，填写一些简单的基本信息之后即可创建。 在机器人设置界面找到你的 API KEY ，记录下来。 在Python中使用图灵机器人API v2.0 基本原理就是使用urllib.request模块，向接口地址发送HTTP POST请求，请求中加入了聊天内容。 *使用python3执行 import json import urllib.request api_url = http://openapi.tuling123.com/openapi/api/v2 text_input = input('我：') req = {  tperception:  t{  t tinputText:  t t{  t t ttext: text_input  t t},  t tselfInfo:  t t{  t t tlocation:  t t t{  t t t tcity: 上海,  t t t tprovince: 上海,  t t t tstreet: 文汇路  t t t}  t t}  t},  tuserInfo:  t{  t tapiKey: 请替换为你的API KEY,  t tuserId: OnlyUseAlphabet  t} } # print(req) # 将字典格式的req编码为utf8 req = json.dumps(req).encode('utf8') # print(req) http_post = urllib.request.Request(api_url, data=req, headers={'content-type': 'application/json'}) response = urllib.request.urlopen(http_post) response_str = response.read().decode('utf8') # print(response_str) response_dic = json.loads(response_str) # print(response_dic) intent_code = response_dic['intent']['code'] results_text = response_dic['results'][0]['values']['text'] print('Turing的回答：') print('code：' + str(intent_code)) print('text：' + results_text)  几点说明： 1、字典 req 包含了向图灵机器人发出请求所需的各项信息。其中 req['perception']['selfInfo']['location'] 包含了地理位置信息，向图灵机器人发送与位置有关的请求时，如果没有另外指定位置，则会默认使用这个位置。例如询问”明天会下雨吗”，图灵机器人会回答我”上海”明天是否下雨。 2、 req['userInfo'] 包含了API KEY，请替换成你的API KEY（双引号不要删除）。另外 userId 是用户参数，暂时不明白用途，如果你有什么想法恳请留言。 3、图灵机器人的回答可以转换为python的字典格式。其中有一项 response_dic['intent']['code'] 官方称为”输出功能code”，表示这个回答是什么”类型”的。例如10004代表普通的聊天回复，10008代表与天气相关的回复。然而奇怪的是，目前API v2.0的官方文档并没有给出code和类型的对照表。目前自己总结了一些如下，欢迎补充：    code  类型     10004  聊天    10008  天气    10013  科普类，例如”班戟是什么”    10015  菜谱类，例如”剁椒鱼头怎么做”    10019  日期类，例如”愚人节是几号”、”明天是星期几”    10020  中英翻译    10023  一般返回网页会是这个code，例如”iphone多少钱”    10034  语料库中自己设定的回答   小结 到现在为止，已经快把每个独立的模块完成了，接下来该准备考虑如何把它们整合在一起了。希望能帮到你。 感谢你阅读文章！ ",
      "url"      : "https://0qinghao.github.io/inforest/2018/03/22/turing-chat-bot-python-API/"
    } ,
  
    {
      "title"    : "在Python中使用科大讯飞Web API进行语音合成",
      "category" : "python",
      "content": "前几日讯飞开放平台推出了WebAPI接口，恰好最近需要实现一个文字转语音的功能，于是就尝试着用了起来。但不知什么原因，官方文档的调用示例一直报错，最后自己照着示例的思路用python3重写了一遍。所以这次总结一下在Python中使用讯飞Web API进行语音合成的过程。 注册讯飞开放平台 首先注册讯飞开放平台：http://passport.xfyun.cn/register 注册完成后进入控制台，在控制台创建一个新应用 ，填写一些基本信息，注意 应用平台 选择 WebAPI 。 创建完成后，记录下 APPID 和 APIKey ，将在程序中用到。 另外，请在 IP白名单 中添加自己的外网IP，可以在http://www.ip138.com/ 查看。（一般来说外网IP会常常发生变化，请注意） 在Python3中使用讯飞Web API 先上代码，后面进行必要的说明： 可能提示缺库：pip3 install requests *使用python3执行 import base64 import json import time import hashlib import requests # API请求地址、API KEY、APP ID等参数，提前填好备用 api_url = http://api.xfyun.cn/v1/service/v1/tts API_KEY = 替换成你的APIKEY APP_ID = 替换成你的APPID OUTPUT_FILE = C://output.mp3 # 输出音频的保存路径，请根据自己的情况替换 TEXT = 苟利国家生死以，岂因祸福避趋之 # 构造输出音频配置参数 Param = {  auf: audio/L16;rate=16000, #音频采样率  aue: lame, #音频编码，raw(生成wav)或lame(生成mp3)  voice_name: xiaoyan,  speed: 50, #语速[0,100]  volume: 77, #音量[0,100]  pitch: 50, #音高[0,100]  engine_type: aisound #引擎类型。aisound（普通效果），intp65（中文），intp65_en（英文） } # 配置参数编码为base64字符串，过程：字典→明文字符串→utf8编码→base64(bytes)→base64字符串 Param_str = json.dumps(Param) #得到明文字符串 Param_utf8 = Param_str.encode('utf8') #得到utf8编码(bytes类型) Param_b64 = base64.b64encode(Param_utf8) #得到base64编码(bytes类型) Param_b64str = Param_b64.decode('utf8') #得到base64字符串 # 构造HTTP请求的头部 time_now = str(int(time.time())) checksum = (API_KEY + time_now + Param_b64str).encode('utf8') checksum_md5 = hashlib.md5(checksum).hexdigest() header = {  X-Appid: APP_ID,  X-CurTime: time_now,  X-Param: Param_b64str,  X-CheckSum: checksum_md5 } # 发送HTTP POST请求 def getBody(text):  data = {'text':text}  return data response = requests.post(api_url, data=getBody(TEXT), headers=header) # 读取结果 response_head = response.headers['Content-Type'] if(response_head == audio/mpeg):  out_file = open(OUTPUT_FILE, 'wb')  data = response.content # a 'bytes' object  out_file.write(data)  out_file.close()  print('输出文件: ' + OUTPUT_FILE) else:  print(response.read().decode('utf8')) 下面按照代码顺序进行各部分的说明。 APIKey等参数 在代码开头填好各项参数，方面代码中使用。 API_KEY和APP_ID请替换为上一步创建应用后得到的内容。请不要删除双引号。 OUTPUT_FILE是最终输出音频的保存路径，根据自己的情况替换。 TEXT是将要输出为语音的文本。 音频配置参数 Param 是字典格式的音频配置参数，其中 aue 可选 raw (生成wav)或 lame (生成mp3)，如果修改成raw请记得同时修改输出文件的扩展名。 最后需要将配置参数编码为Base64字符串：字典类型→明文字符串→utf8编码→Base64(bytes)→Base64字符串，具体实现可以参考代码。 音频配置参数的详细说明可以参考请求参数 - 语音合成 。 HTTP请求头部 根据 授权认证 - 科大讯飞RESET_API开发指南 ，在调用所有业务接口时，都需要在HTTP请求头部中配置以下参数用于授权认证：    参数  格式  说明     X-Appid  string  讯飞开放平台注册申请应用的应用ID(appid)    X-CurTime  string  当前UTC时间戳，从1970年1月1日0点0 分0 秒开始到现在的秒数    X-Param  string  音频配置参数JSON串经Base64编码后的字符串    X-CheckSum  string  令牌，计算方法：MD5(apiKey + curTime + param)。三个值拼接的字符串，进行MD5哈希计算（32位小写）。   具体实现参考代码中字典 header 。 发送请求&amp;读取结果 最后使用requests库发送HTTP POST请求，得到结果。根据响应的 header 可以判断是否合成成功。 若响应头部包含 Content-type: audio/mpeg ，则响应Body为音频数据，可写入文件保存。 若合成出现错误，响应头部包含 Content-type: text/plain ，响应Body为记载了错误类型的json字符串。 返回值的具体说明请参考 返回值 - 语音合成 。 运行结果 使用几次后，感觉合成语音的断句做得不是很优秀，但响应速度很快，还是比较满意的。 output.mp3 小结 最近使用了几种Web API，对这类API的使用方法也算是有些经验了。最后，现在语音识别、图灵机器人、语音合成都试着做了一遍，下一篇博客将把他们组合起来，实现一个简单的语音助手。 感谢你阅读文章！ ",
      "url"      : "https://0qinghao.github.io/inforest/2018/03/24/xunfei-tts-web-api-python/"
    } ,
  
    {
      "title"    : "使用Python把树莓派改造成一个语音助手",
      "category" : "python",
      "content": "语音助手已经不是什么新事物了。就在两三年前，语音助手的使用体验还不是那么好，尝尝鲜后也就没用过了。但最近发现不管是微软的Cortana、苹果的Siri，还是一些不怎么有名气的，例如MIUI的小爱同学等，使用体验真的改善了很多，确确实实能带来一些方便了。 随着各种云服务、API的面世，语音方面的云服务可以说是十分健全了。你是否也想过自己动手搭建一个语音助手系统呢？本文将总结使用Python把 树莓派（3代b型） 改造成一个简易语音助手的基本流程。 概述 这次要做的说白了，就是把各种云服务、API串起来，并不涉及任何核心技术、算法的实现，望知悉。 这次将要使用到的服务包括：  谷歌Cloud Speech API 图灵机器人 科大讯飞 语音合成WebAPI 为了实现这个语音助手系统，需要完成的工作每一个都不难，但数量稍多了些。以下是涉及到的一些博客：  使用Google云计算引擎实现科学上网 在Windows命令行、Linux终端使用代理 树莓派学习手记——使用Python录音 在Python中使用谷歌Cloud Speech API将语音转换为文字（另一种方案） 使用Python与图灵机器人聊天 在Python中使用科大讯飞Web API进行语音合成 后文在介绍各部分的具体实现时，只附上代码和进行一些必要的说明，详细内容还需要参考相应博客。 各部分的实现 由于整个项目用到的服务比较多，而且各部分的分工很明显，所以选择各部分分别用一个python程序来实现，最后再用一个程序整合在一起的方式。 录音 参考：树莓派学习手记——使用Python录音 笔者采用了“按住按钮进行录音”的操作方式，如下图所示接线。如果你手头上没有按钮或觉得这么做不方便，可以修改代码改成“按回车键开始/结束录音”之类的操作方式。 另外，树莓派的板载3.5mm耳机接口是不带语音输入功能的，所以你需要另外购买USB声卡。  文件 rec.py import RPi.GPIO as GPIO import pyaudio import wave import os import sys def rec_fun():  t# 隐藏错误消息，因为会有一堆ALSA和JACK错误消息，但其实能正常录音  tos.close(sys.stderr.fileno())  t  tBUTT = 26 t# 开始录音的按钮：一边接GPIO26，一边接地  tGPIO.setmode(GPIO.BCM)  t# 设GPIO26脚为输入脚，电平拉高，也就是说26脚一旦读到低电平，说明按了按钮  tGPIO.setup(BUTT, GPIO.IN, pull_up_down = GPIO.PUD_UP)  t# wav文件是由若干个CHUNK组成的，CHUNK我们就理解成数据包或者数据片段。  tCHUNK = 512  tFORMAT = pyaudio.paInt16 # pyaudio.paInt16表示我们使用量化位数 16位来进行录音  tRATE = 44100 # 采样率 44.1k，每秒采样44100个点。  tWAVE_OUTPUT_FILENAME = /home/pi/chat/command.wav  tprint('请按住按钮开始录音...')  tGPIO.wait_for_edge(BUTT, GPIO.FALLING)  t# To use PyAudio, first instantiate PyAudio using pyaudio.PyAudio(), which sets up the portaudio system.  tp = pyaudio.PyAudio()  tstream = p.open(format = FORMAT,  t t t t tchannels = 1, t# cloud speecAPI只支持单声道  t t t t trate = RATE,  t t t t tinput = True,  t t t t tframes_per_buffer = CHUNK)  tprint(录音中...)  tframes = []  t# 按住按钮录音，放开时结束  twhile GPIO.input(BUTT) == 0:  t tdata = stream.read(CHUNK)  t tframes.append(data)  tprint(录音完成，输出文件： + WAVE_OUTPUT_FILENAME + '  ')  tstream.stop_stream()  tstream.close()  tp.terminate()  twf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')  twf.setnchannels(1)  twf.setsampwidth(p.get_sample_size(FORMAT)) t# Returns the size (in bytes) for the specified sample format.  twf.setframerate(RATE)  twf.writeframes(b''.join(frames))  twf.close()  t  treturn # 可以直接运行rec.py进行测试，同时保证该文件import时不会自动运行 if __name__ == '__main__':  trec_fun() 语音识别 参考： 使用Google云计算引擎实现科学上网 在Windows命令行、Linux终端使用代理 在Python中使用谷歌Cloud Speech API将语音转换为文字（另一种方案） 由于某些原因，笔者选择了使用谷歌Cloud Speech API进行语音识别。既然要用谷歌的服务，自然就涉及到了科学上网、代理、谷歌云平台的使用，如果不想这么折腾，完全可以用国内的讯飞、百度来实现。 另外，API KEY之类的字符串在这里删除了，还请麻烦修改代码加上你自己申请的API KEY。  文件 speech_api.py import json import urllib.request import base64 def wav_to_text():  tapi_url = https://speech.googleapis.com/v1beta1/speech:syncrecognize?key=替换成你的API密钥  tprint('语音文件编码中...')  taudio_file = open('/home/pi/chat/command.wav', 'rb')  taudio_b64str = (base64.b64encode(audio_file.read())).decode()  taudio_file.close()  tvoice = {  t config:  t {  t tlanguageCode: cmn-Hans-CN  t },  t audio:  t {  t tcontent: audio_b64str  t }  t}  tvoice = json.dumps(voice).encode('utf8')  tprint('编码完成。正在上传语音...')  treq = urllib.request.Request(api_url, data=voice, headers={'content-type': 'application/json'})  tresponse = urllib.request.urlopen(req)  tresponse_str = response.read().decode('utf8')  tresponse_dic = json.loads(response_str)  tif ('results' not in response_dic.keys()):  t tprint('您录制的文件似乎没有声音，请检查麦克风。')  t treturn  t  ttranscript = response_dic['results'][0]['alternatives'][0]['transcript']  tconfidence = response_dic['results'][0]['alternatives'][0]['confidence']  tresult_dic = {'text':transcript ,'confidence':confidence}  tprint('识别完成。以字典格式输出：')  tprint(result_dic)  t  treturn result_dic if __name__ == '__main__':  twav_to_text() 获取文字回答 参考：使用Python与图灵机器人聊天 这个获取回答的程序有些粗糙，只能获得普通的文字回答。实际上图灵机器人回复的内容中包括了文字、问题类型甚至情感等信息，还有很多修改的空间。  文件 turing.py import json import urllib.request def chat(question):  tapi_url = http://openapi.tuling123.com/openapi/api/v2  ttext_input = question['text']  treq = {  t tperception:  t t{  t t tinputText:  t t t{  t t t ttext: text_input  t t t},  t t tselfInfo:  t t t{  t t t tlocation:  t t t t{  t t t t tcity: 上海,  t t t t tprovince: 上海,  t t t t tstreet: 文汇路  t t t t}  t t t}  t t},  t tuserInfo:  t t{  t t tapiKey: 替换成你的APIKEY,  t t tuserId: 用户参数  t t}  t}  t# 将字典格式的req转为utf8编码的字符串  treq = json.dumps(req).encode('utf8')  t  tprint('  ' + '正在调用图灵机器人API...')  thttp_post = urllib.request.Request(api_url, data=req, headers={'content-type': 'application/json'})  tresponse = urllib.request.urlopen(http_post)  t  tprint('得到回答，输出为字典格式：')  tresponse_str = response.read().decode('utf8')  tresponse_dic = json.loads(response_str)  tintent_code = response_dic['intent']['code']  t  t# 返回网页类的输出方式  tif(intent_code == 10023):  t tresults_url = response_dic['results'][0]['values']['url']  t tresults_text = response_dic['results'][1]['values']['text']  t tanswer = {code: intent_code, text: results_text, url:results_url}  t tprint(answer)  t treturn(answer)  t# 一般的输出方式  telse:  t tresults_text = response_dic['results'][0]['values']['text']  t tanswer = {code: intent_code, text: results_text}  t tprint(answer)  t treturn(answer)  t if __name__ == '__main__':  teg_question = {'text': '今天是几号', 'confidence': 0.9}  tchat(eg_question) 读出回答（语音合成） 参考：在Python中使用科大讯飞Web API进行语音合成 笔者在使用讯飞Web API时，该服务才开放不到一周，难免以后该API会有所变动，如有问题建议查阅官方文档。  文件 tts.py import base64 import json import time import hashlib import urllib.request import urllib.parse import os def speak(text_content):  t# API请求地址、API KEY、APP ID等参数，提前填好备用  tapi_url = http://api.xfyun.cn/v1/service/v1/tts  tAPI_KEY = 替换成你的APIKEY  tAPP_ID = 替换成你的APPID  tAUE = lame  t# 构造输出音频配置参数  tParam = {  t tauf: audio/L16;rate=16000, t#音频采样率  t taue: AUE, t#音频编码，raw(生成wav)或lame(生成mp3)  t tvoice_name: xiaoyan,  t tspeed: 50, t#语速[0,100]  t tvolume: 10, t#音量[0,100]  t tpitch: 50, t#音高[0,100]  t tengine_type: aisound t#引擎类型。aisound（普通效果），intp65（中文），intp65_en（英文）  t}  t# 配置参数编码为base64字符串，过程：字典→明文字符串→utf8编码→base64(bytes)→base64字符串  tParam_str = json.dumps(Param) t#得到明文字符串  tParam_utf8 = Param_str.encode('utf8') t#得到utf8编码(bytes类型)  tParam_b64 = base64.b64encode(Param_utf8) t#得到base64编码(bytes类型)  tParam_b64str = Param_b64.decode('utf8') t#得到base64字符串  t# 构造HTTP请求的头部  ttime_now = str(int(time.time()))  tchecksum = (API_KEY + time_now + Param_b64str).encode('utf8')  tchecksum_md5 = hashlib.md5(checksum).hexdigest()  theader = {  t tX-Appid: APP_ID,  t tX-CurTime: time_now,  t tX-Param: Param_b64str,  t tX-CheckSum: checksum_md5  t}  t# 构造HTTP请求Body  tbody = {  t ttext: text_content  t}  tbody_urlencode = urllib.parse.urlencode(body)  tbody_utf8 = body_urlencode.encode('utf8')  t# 发送HTTP POST请求  tprint('  ' + 正在调用科大讯飞语音合成API...)  treq = urllib.request.Request(api_url, data=body_utf8, headers=header)  tresponse = urllib.request.urlopen(req)  t# 读取结果  tresponse_head = response.headers['Content-Type']  tif(response_head == audio/mpeg):  t tout_file = open('/home/pi/chat/answer.mp3', 'wb')  t tdata = response.read() # a `bytes` object  t tout_file.write(data)  t tout_file.close()  t tprint('得到结果，输出文件: /home/pi/chat/answer.mp3')  telse:  t tprint(response.read().decode('utf8'))  t# 播放音频  tprint(播放音频中...)  tprint(以下均为mplayer的输出内容  )  tos.system(mplayer -ao alsa:device=hw=1.0 /home/pi/chat/answer.mp3)  t  treturn  t if __name__ == '__main__':  teg_text_content = 苟利国家生死以，岂因祸福避趋之  tspeak(eg_text_content) 整合&amp;测试 现在，你的项目文件夹中应该有这些python代码文件： 接下来我们只需要将他们整合在一起运行。  文件 combine.py # 这些import进来的模块是同目录下的py文件 import rec t# rec.py负责录制wav音频 import speech_api t# speech_api.py负责wav转文字 import turing t# turing.py负责获得图灵机器人的文字回答 import tts t# tts.py负责读出回答 rec.rec_fun() t# 录制音频 recognize_result = speech_api.wav_to_text() t# 识别语音，返回值是字典格式，包含文字结果和信心 turing_answer = turing.chat(recognize_result) t# 得到图灵的回答，返回值仍是字典格式 tts.speak(turing_answer['text']) 如果一切顺利的话，实际运行效果如下： 树莓派_语音助手_youku 小结 语音助手这边的工作算是告一段落了，结果小结却不知道怎么写了。不管怎么说，很开心最后能得到实际的结果，做的过程中也有一些脑洞想要继续扩展，过段时间应该还会继续！ 做这个项目的过程中，项目外的收获或许比这个项目本身还要多。这段时间从很多博客、论坛得到了数不尽的帮助，国内的、国外的、中文的、英文的、日文的都有，深深地感受到了互联网共享精神的力量，这也是促使我开始写这些文章的原因。那么，最后还是说一句：感谢你阅读文章！ ",
      "url"      : "https://0qinghao.github.io/inforest/2018/03/26/PiDaXing/"
    } ,
  
    {
      "title"    : "欧拉角R、P、Y的直观说明",
      "category" : "数学",
      "content": "Roll: 横滚 Pitch: 俯仰 Yaw: 偏航（航向）  ",
      "url"      : "https://0qinghao.github.io/inforest/2018/03/28/eular-angle-anime-show/"
    } ,
  
    {
      "title"    : "python - 创建virtualenv时选择继承系统站点程序包",
      "category" : "python",
      "content": "笔记 virtualenv --system-site-packages --python python3 env 可以解决smbus模块不存在的问题。 ",
      "url"      : "https://0qinghao.github.io/inforest/2018/03/28/python-virtualenv-inherit/"
    } ,
  
    {
      "title"    : "树莓派学习手记——制作一个空调遥控器（红外接收、发射的实现）",
      "category" : "raspberrypi",
      "content": "使用树莓派搭配红外管，进行接收、发射红外信号是很方便的，同时红外信号也有很广泛的用途。这次我们将总结使用树莓派制作一个空调红外遥控器的过程。 准备工具  红外接收管（参考型号HS0038B） 红外发射管（参考型号TSAL6200） 遥控器（或能使用万能遥控器的手机） 用作开关的三极管、限流电阻（非必须、参考型号S9013）  使用开关三极管可以有效增强红外发射管的性能，但不是必须的。不使用三极管也能在三五米范围内成功遥控空调。这些材料总共费用不超过1块钱，反而是快递费比较贵了。 看到遥控器、接收管、发射管，相信已经有人明白了制作遥控器的原理。是的，我们只需要事先把遥控器发射出的红外信号记录下来，然后通过树莓派依样画葫芦地把这个信号发射出去，一个“克隆”版的遥控器就做好了。 硬件连接 *注意：两个GPIO引脚是固定的，与后续安装的软件有关。 接收管信号输出脚 OUT → GPIO18 发射管正极（不使用开关三极管的情况下） → GPIO17 如果你手头上没有开关三极管，直接将红外发射管正极接在GPIO17，如下所示： 如果接入三极管，用GPIO17连接基极，控制发射极和集电极的通断： （偷懒了没有接入限流电阻，在意的同学自行接入） 安装lirc  解决方案来自：LIRC: Linux Infrared Remote Control for Raspberry Pi sudo apt update sudo apt install lirc 修改CONFIG. TXT 修改文件 /boot/config.txt ： sudo nano /boot/config.txt 找到 lirc-rpi module 的部分，修改为： # Uncomment this to enable the lirc-rpi module dtoverlay=lirc-rpi,gpio_out_pin=17,gpio_in_pin=18,gpio_in_pull=up ！！！注意：config.txt的配置内容，似乎根据不同Linux内核版本有微妙的变化，手头上暂时没有其他平台可以测试。如果后续测试时出问题，请Google关键词“lirc lirc-rpi gpio-ir”查阅相关资料。 修改驱动配置 修改文件 /etc/lirc/lirc_options.conf ： sudo nano /etc/lirc/lirc_options.conf # 把： driver = devinput device = auto # 修改为： driver = default device = /dev/lirc0 最后，重启树莓派。 简单测试是否正常 # 必须停止lircd服务才能进入接收红外信号模式 sudo service lircd stop mode2 -d /dev/lirc0 运行上述命令后，用遥控器对着接收管随便按一些按钮，如果出现形式如下的输出就表示正常： space 16777215 pulse 8999 space 4457 pulse 680 space 1627 ...... 录入红外信号  解决方案来自：How to Control Your Air Conditioner with Raspberry Pi Board and ANAVI Infrared pHAT lirc有一个自动录入红外信号、生成遥控器文件的功能。但此方法只适用于简单设备，比如风扇，这里就不记录过程了。有需要的直接运行 irrecord -d /dev/lirc0 --disable-namespace ，按提示做完后把生成的文件放到 /etc/lirc/lircd.conf.d/ 目录就行了。 这边就主要针对空调这种复杂设备，记录录入红外信号的过程。 另外，简单了解一下红外NEC协议可以帮助你理解配置的过程。 为什么无法直接录制复杂设备的红外控制信号？ 因为空调遥控器每次发送的信号不是单纯的一个”byte”，与其说它是“控制信号”，不如说是一个“状态”、“情景”。后文还会有实例帮助你理解。 生成遥控器配置文件的样板 空调这类复杂设备的遥控器配置文件，是需要自己手动输入的。但不可能整个文件都自己写——我们连格式都不知道。 所以我们需要用刚才提到的自动录入功能生成一个样板，但请记住，这个样板中记录的信号极可能是不正确的！我们只是通过它来了解配置内容的格式。 开始自动录制： # 请cd到有读写权限的目录下，因为需要创建一个遥控器配置文件 # 参数-f --force 表示 Force raw mode irrecord -f -d /dev/lirc0 --disable-namespace 认真阅读提示信息，根据提示按Enter、输入 遥控器名称 、按Enter、按照要求随机按遥控器、输入 按钮名称 、按对应的遥控器按钮。由于只是为了生成样板，所以录制一个按钮就够了。完成录制后，当前目录下会生成一个遥控器配置文件 遥控器名称.lircd.conf 。 如果发现录制过程十分缓慢，最后提示“未发现gap”之类的信息，请尝试跳过自动生成这一步，复制下面的配置文件当做生成的配置，直接进入下一步。（我在录制一些老式空调的命令时遇到了这种问题，只能这样解决，如果你有什么想法恳请提出） 我在录制时输入的 遥控器名称 是aircon，录制的一个按钮是on，所以配置文件的内容形式如下： begin remote name aircon flags RAW_CODES eps  30 aeps  100 gap  19991  begin raw_codes  name on   9042 4438  700 1602  705  526   678  528  681  531  674  527   679  528  679  528  677  527   677  528  679  528  678  528   677 1632  676  529  676  531   676  531  649  556  672  532   650  558  654  552  652  553   649  558  648 1661  650  558   648  558  648 1661  649  562   644  558  647  558  648 1657   651  558  647 1659  650  557   653  553  648 1660  648  557   649  end raw_codes end remote 如果你阅读了红外NEC协议，就能马上意识到，这一串数字其实就是红外信号脉冲(pulse)、空白(space)的持续时间。 手动编辑遥控器配置文件 打开刚才生成的样板文件 遥控器名称.lircd.conf ，很容易发现 begin raw_codes 和 end raw_codes 之间的内容就是需要我们手动修改的内容。刚才也提到过，样板中记录的信号极可能是不正确的，所以我们先把自动生成的 on 按钮下方的信号数据删除掉。 还记得刚才测试时使用的mode2命令吗。我们现在需要做的就是使用mode2命令接收遥控器发出的信号，然后将其加入到文件 遥控器名称.lircd.conf 中。首先，我们来录入正确的 on 按钮的信号数据： # -m --mode 使用行列显示模式，不显示pulse、space mode2 -m -d /dev/lirc0 按下遥控器上的“开”按钮，得到形式如下的输出： 16777215  9059  4432 706  1604 706 528  679 524 681  1603 703 526  680  1602 715  1596 704 526  679 527 679 527 680 527  679  1604 705 530 673 530  674 529 682 529 675 530  674 532 674 532 650 557  648 556 654  1653 676 533  649 559 647  1667 639 559  648 558 656 553 647  1658  648 558 650  1659 649 559  647 559 648  1659 648 558  646 19991  648 558 648 558 650 567  638 557 648  1668 640 557  649 558 650 558 646  1660  650 556 649 557 649 559  654 552 648  1657 651 558  647 554 660 549 649 559  647 557 649 559 648 559  647 557 644 561 648 559  648 556 647 560 648 556  652 563 642  1658 648  1661  649  1660 646  1658 650 除去第一行很大的那个数，把其他数据全部复制，粘贴到配置文件的 name on 下方。例如现在我必须删除“16777215”这个数，剩下的内容粘贴到配置文件的 name on 下方。 重复上述操作，增加更多的按钮，例如 name off 、 name 26C 等。最后我录制了3个按钮，配置文件编辑成了这样： begin remote name aircon flags RAW_CODES eps  30 aeps  100 gap  19991  begin raw_codes  name on  t t t 9059  4432 706  1604 706 528  t t t 679 524 681  1603 703 526  t t t 680  1602 715  1596 704 526  t t t 679 527 679 527 680 527  t t t 679  1604 705 530 673 530  t t t 674 529 682 529 675 530  t t t 674 532 674 532 650 557  t t t 648 556 654  1653 676 533  t t t 649 559 647  1667 639 559  t t t 648 558 656 553 647  1658  t t t 648 558 650  1659 649 559  t t t 647 559 648  1659 648 558  t t t 646 19991  t t t 648 558 648 558 650 567  t t t 638 557 648  1668 640 557  t t t 649 558 650 558 646  1660  t t t 650 556 649 557 649 559  t t t 654 552 648  1657 651 558  t t t 647 554 660 549 649 559  t t t 647 557 649 559 648 559  t t t 647 557 644 561 648 559  t t t 648 556 647 560 648 556  t t t 652 563 642  1658 648  1661  t t t 649  1660 646  1658 650  t t  t t name off  t t t 9029  4432 715  1594 706 526  t t t 682 523 681 525 680 526  t t t 681  1601 708  1607 699 524  t t t 688 519 682 526 678 527  t t t 681  1601 708 524 687 520  t t t 682 525 677 527 677 529  t t t 675 531 676 531 674 532  t t t 651 558 646  1659 650 557  t t t 648 557 650  1659 653 554  t t t 650 559 647 558 649  1657  t t t 649 558 648  1661 648 557  t t t 646 562 645  1666 643 558  t t t 649 19992  t t t 651 555 650 558 648 562  t t t 645 557 648  1661 653 552  t t t 646 560 650 557 648  1657  t t t 649 561 647 557 647 558  t t t 650 556 650  1659 649 559  t t t 647 557 649 558 648 559  t t t 647 557 651 564 642 559  t t t 646 557 649 557 657 552  t t t 647 557 648 558 650 557  t t t 645 560 653  1653 646  1661  t t t 650  1659 648 558 647  t t name 26C  t t t 9026  4430 705  1604 706 528  t t t 679 535 670  1604 705 527  t t t 675 532 679  1607 702 530  t t t 673 531 683  1625 672 535  t t t 672  1633 676 530 673 534  t t t 649 558 648 563 642 556  t t t 651 556 650 558 672 532  t t t 649 556 652  1659 648 558  t t t 656 551 646  1659 650 558  t t t 648 558 648 558 649  1658  t t t 649 561 648  1659 647 559  t t t 650 556 648  1660 646 559  t t t 647 19990  t t t 648  1659 649 558 648 558  t t t 647 558 650  1658 650 557  t t t 650 555 650 558 648 558  t t t 649 555 652 561 667 534  t t t 648 559 648  1658 656 550  t t t 650 557 672 533 649 555  t t t 650 559 649 558 647 559  t t t 648 558 648 566 641 558  t t t 647 558 648 558 650 558  t t t 648 558 648  1660 646 558  t t t 648 558 646 562 647  end raw_codes end remote 是的，如果你想要实现完整的控制，你就需要把所有按钮都录制一遍。如果你对配置文件中开头的eps、aeps等参数感兴趣，或者最后遥控不太正常，阅读lircd.conf manual或许能帮到你。我使用的是默认的数值，一切工作正常。 最后，把配置文件复制到指定目录 /etc/lirc/lircd.conf/ 并重启lircd服务： sudo cp aircon.lircd.conf /etc/lirc/lircd.conf.d/ sudo service lircd restart *后续步骤出现问题的同学可以使用service lircd status查看服务启动的log，帮助定位bug。 发射信号 终于，我们可以尝试着使用树莓派控制空调了。如果你没有使用开关三极管，你可能需要把树莓派拿到靠近空调的地方，并且把红外发射管对准空调。如果你使用了三极管，那么注意树莓派和空调之间不要有明显的物体阻隔即可。 # 发射命令：irsend SEND_ONCE 遥控器名称 按钮名称 irsend SEND_ONCE aircon on 如果前面的步骤一切正常，但在发射信号时报错“transmission failed”。请检查生成的遥控器配置文件，查看flags项，若是 flags RAW_CODES|CONST_LENGTH ，请尝试将其修改成 flags RAW_CODES 并重启lircd服务。再测试能否发射信号。 按钮？不如说是情景 最后，我们来讨论一个比较有意思的东西。 考虑一下这种情况：我为了录入 + 按钮，运行mode2命令开始录制。在遥控器显示温度23℃时按 + ，然后按照前面的方法编辑配置文件，写入了按钮 name add 。 此时空调屏幕上显示温度是24℃。提问：如果我运行 irsend SEND_ONCE aircon add 空调会：  温度提升到25℃ “滴”地响一声，然后什么都没发生，保持在24℃ 很遗憾，后者发生了。 实际上遥控器每按下一次按钮发送的信息是一个“情景”，我刚才录制的 add 按钮实际上是表示“温度设为24℃、进入制冷模式、风速设为自动…”这样的一个“情景”。如果你在空调温度20℃时运行add命令，那么它就会一次性提升到24℃！ 这意味着，如果你想要设置任意温度，你需要把每一度都录制一遍，因为 + 、 - 命令根本就不存在。 当然，这也不全是坏事。 我录制了一个按钮 26C ，功能是将温度调到26℃。然后我意识到， 26C 这个按钮同时包含了开关状态的信息。是的！在空调关闭的情况下，如果我直接发送命令： irsend SEND_ONCE aircon 26C 那么空调会打开，并且调整到26℃！ 于是，我录制了一个按钮 Sleep ，它将空调设置为“26℃、风速设为低、开启扫风、开启静音睡眠模式”。睡前运行一次 irsend SEND_ONCE aircon Sleep ，感觉离智能家居又近了一步 23333 (•̀ω•́)✧。 小结 其实写完这篇总结还是有点慌的，因为不管是树莓派版本、软件版本、红外管型号还是空调的型号，大家都是不一样的，说不准哪一步我这么做放别人那就是错的呢。事实上，我自己在做的过程中参考的一些博客就和我的实际情况有些出入了。只能希望这篇总结能够有一定的参考价值。最后，感谢你阅读文章！ ",
      "url"      : "https://0qinghao.github.io/inforest/2018/04/02/raspberrypi-aircon-ir-remote/"
    } ,
  
    {
      "title"    : "常见内网网段",
      "category" : "network",
      "content": " 内网IP段有哪些 常见的内网IP段有： 10.0.0.0/8 10.0.0.0 - 10.255.255.255 172.16.0.0/12 172.16.0.0 - 172.31.255.255 192.168.0.0/16 192.168.0.0 - 192.168.255.255 以上三个网段分别属于A、B、C三类IP地址，来自 《RFC 1918》。 但是根据 《Reserved IP addresses - Wikipedia, the free encyclopedia》 及《RFC 6890 - Special-Purpose IP Address Registries》的描述， 还有很多其它的内网IP段（包括IPv6），以及其它用途的保留IP地址。 其它IPv4内网段罗列如下： 0.0.0.0/8 0.0.0.0 - 0.255.255.255 用于当前网络内的广播消息。 100.64.0.0/10 100.64.0.0 - 100.127.255.255 由运营商使用的私网IP段，随着IPv4地址池的耗光，会有更多用户被分配到这个网段。 127.0.0.0/8 127.0.0.0 - 127.255.255.255 本机回环地址。 169.254.0.0/16 169.254.0.0 - 169.254.255.255 获取不到IP地址时使用，通常因为从DHCP服务器获取不到IP。 255.255.255.255/32 255.255.255.255 本网段的广播地址。 ",
      "url"      : "https://0qinghao.github.io/inforest/2018/04/05/common-local-ip/"
    } ,
  
    {
      "title"    : "译 - 使用谷歌Cloud Translation API翻译文本",
      "category" : "google cloud",
      "content": " 原文：Translation APIでテキストをほんやくする 概要 使用Cloud Translation，可以将任意的字符串翻译为API支持的语言。由于语言检测功能的存在，即使在源语言未知的情况下，也能使用该API。 将要学习的东西  创建Cloud Translation API请求，使用curl调用API 翻译文本的方法 高级版（Premium Edition）的使用方法 检测语言 必要的准备  创建Google Cloud Platform项目 浏览器（Chrome、Firefox 等） 设置和一些说明 根据自己的情况进行设置 还未拥有Google账号（Gmail / Google Apps）的情况下，创建账号是必须的。登录Google Cloud Platform Console（console.cloud.google.com），创建一个新项目。 请记住项目名称。任意一个Google Cloud项目都拥有唯一的名称（上述的名称已经被使用了，所以实际上无法使用）。 Google Cloud Platform的新用户将赠与相当于$ 300的试用金。 用于教学的账号 教师将已有的项目进行设置，生成临时账号。若你得到了教师发放的临时账号，你将不必顾虑项目中产生的费用。但是，一旦该教学项目结束，所有的临时账号将被无效化。 当你从教师那收到了临时账号的用户名/密码，就能够登录Google Cloud Console（https://console.cloud.google.com/）。 登录后，你将看到如下界面。 启用Translation API 点击屏幕左上角的菜单图标。 在下拉菜单中选择 [API Manager] 点击 [启用API] 。 然后，在搜索框中输入「translate」。点击 [Google Cloud Translation API]。 API已经启用的情况下，只会显示 [停用] 按钮。请不要停用API。 API还未启用的情况下，点击[启用]，启用 Cloud Translation API 。 等待数秒，API成功启用后，将显示如下。 激活Cloud Shell Google Cloud Shell 是在云端运行的命令行环境。这台基于 Debian 的虚拟机能够加载任何您需要的开发工具（gcloud、bq、git等），并提供永久的5 GB主目录。这次教程将使用 Cloud Shell 创建对 Translation API 的请求。 点击标题栏右侧的 [激活 Google Cloud Shell] 按钮（&gt;_），启动Cloud Shell。 Cloud Shell 将在控制台底部的新窗口中打开，并显示命令行提示符。请等待提示符 user@project:~$ 出现。 生成API Key 你将通过使用curl发送一个请求来调用 Translation API 。在发送请求时，你需要在 URL 中插入一个生成的 API 密钥。为了创建 API 密钥，让我们点击侧边栏的 [API Manager] 。 然后，在 [凭据] 选项卡中点击 [创建凭据] 。 在下拉菜单中选择 [API 密钥] 。 最后，复制生成好的密钥。 将密钥复制到剪贴板后，使用下述命令将其保存到 Cloud Shell 的环境变量中。下述的 YOUR_API_KEY 请替换成剪贴板中的内容。 export API_KEY=YOUR_API_KEY 翻译文本 在此例中，将「My name is Steve」这个字符串翻译为西班牙语。使用下述的curl命令，将之前保存好的 API 密钥环境变量和将要翻译的文本一起，传递给 Translation API 。 TEXT=My%20name%20is%20Steve curl https://translation.googleapis.com/language/translate/v2?target=es&amp;key=${API_KEY}&amp;q=${TEXT} 你将得到形式如下的响应。 { data: {  translations: [  {  translatedText: Mi nombre es Steve,  detectedSourceLanguage: en  }  ] } } 响应中，你可以看到翻译出的文本和 API 检测到的源语言。  Premium 模式 Google Cloud Translation 在几乎所有翻译任务中都是用了 Standard Edition 模式。然而 Google 从最近开始，使用了更为强大的 Neural machine Translation System 来优化翻译服务。在这里，我们可以使用 Premium 模式。详情请参阅此处的指南。 检测语言 除了文本翻译以外，Translation API 还能用来检测文本的语言。此例中，我们将检测两个字符串的语言。下面将使用 curl 命令，把之前保存的 API 密钥环境变量和待检测的文本一起传递给 Translation API 。 TEXT_ONE=Meu%20nome%20é%20Steven TEXT_TWO=日本のグーグルのオフィスは、東京の六本木ヒルズにあります curl https://translation.googleapis.com/language/translate/v2/detect?key=${API_KEY}&amp;q=${TEXT_ONE}&amp;q=${TEXT_TWO} 你将得到形式如下的响应。 { data: {  detections: [  [  {   confidence: 0.84644311666488647,   isReliable: false,   language: pt  }  ],  [  {   confidence: 1,   isReliable: false,   language: ja  }  ]  ] } } 本例中返回的语言是 「pt」和「ja」。它们是 ISO-639-1 的标识符，指葡萄牙语和日本语。关于可能的返回值，在Translation API 支持的语言一览中可以查询。 恭喜！ 在此次向导中，我们学习了如何使用 Cloud Translation API 进行文本的翻译。 学到的东西  创建Cloud Translation API请求，使用curl调用API 翻译文本的方法 高级版（Premium Edition）的使用方法 检测语言 下一步  通过常用的编程语言，使用客户端库，学习 Translation API 的示例应用程序。 尝试使用 Vision API 、Speech API 。 ",
      "url"      : "https://0qinghao.github.io/inforest/2018/04/05/google-translation-api/"
    } ,
  
    {
      "title"    : "译 - 使用Natural Language API分析文本的实体与情感",
      "category" : "google cloud",
      "content": " 原文：Natural Language APIでエンティティと感情を分析する 概要 使用 Cloud Natural Language API ，可以从文本中提取实体、分析情感、解析文本构成。 此次向导中，我们将针对 Natural Language API 的3个方法： analyzeEntities 、 analyzeSentiment 和 annotateText 进行学习。 将要学习的东西  构造 Natural Language API 请求，并使用 curl 发送请求 使用 Natural Language API 提取文本中的实体，并进行情感分析 使用 Natural Language API 对文本进行语言分析（语法、词性等） 使用不同的语言构造 Natural Language API 请求 必要的准备  创建Google Cloud Platform项目 浏览器（Chrome、Firefox 等） 设置和一些说明 根据自己的情况进行设置 还未拥有Google账号（Gmail / Google Apps）的情况下，创建账号是必须的。登录Google Cloud Platform Console（console.cloud.google.com），创建一个新项目。 请记住项目名称。任意一个Google Cloud项目都拥有唯一的名称（上述的名称已经被使用了，所以实际上无法使用）。 Google Cloud Platform的新用户将赠与相当于$ 300的试用金。 启用Cloud Natural Language API 点击屏幕左上角的菜单图标。 在下拉菜单中选择 [API Manager] 点击 [启用API] 。 然后，在搜索框中输入「Language」。点击 [Google Cloud Natural Language API]。 点击[启用]，启用 Cloud Natural Language API 。 等待数秒，API成功启用后，将显示如下。 激活Cloud Shell Google Cloud Shell 是在云端运行的命令行环境。这台基于 Debian 的虚拟机能够加载任何您需要的开发工具（gcloud、bq、git等），并提供永久的5 GB主目录。这次教程将使用 Cloud Shell 创建对 Translation API 的请求。 点击标题栏右侧的 [激活 Google Cloud Shell] 按钮（&gt;_），启动Cloud Shell。 Cloud Shell 将在控制台底部的新窗口中打开，并显示命令行提示符。请等待提示符 user@project:~$ 出现。 生成API Key 你将通过使用curl发送一个请求来调用 Natural Language API 。在发送请求时，你需要在 URL 中插入一个生成的 API 密钥。为了创建 API 密钥，让我们点击侧边栏的 [API Manager] 。 然后，在 [凭据] 选项卡中点击 [创建凭据] 。 在下拉菜单中选择 [API 密钥] 。 最后，复制生成好的密钥。此密钥将在向导的后半部分中用到。 你已获得API密钥，我们将其保存在环境变量中，以便每次调用API时不需要重复插入API密钥值。你可以将密钥保存在 Cloud Shell 中，下述的 &lt;your_api_key&gt; 请替换成之前复制的内容。 export API_KEY=&lt;YOUR_API_KEY&gt; 构造分析文本中实体的请求 第一个介绍的 Natural Language API 方法是 analyzeEntities 。API 使用此方法从文本中提取出实体（人物、场所、事件等）。为了试用 API 的实体分析功能，我们将引用最近新闻中的以下句子。 LONDON — J. K. Rowling always said that the seventh Harry Potter book, “Harry Potter and the Deathly Hallows, “ would be the last in the series, and so far she has kept to her word. 对 Natural Language API 发出的请求可以事先保存在 request.json 文件中。首先，我们在 Cloud Shell 中生成这个文件。 touch request.json 然后，使用任意一个文本编辑器（ nano 、 vim 、 emacs ）打开生成的文件。在文件 request.json 中添加如下内容。 request.json { document:{  type:PLAIN_TEXT,  content:LONDON — J. K. Rowling always said that the seventh Harry Potter book, ‘Harry Potter and the Deathly Hallows,' would be the last in the series, and so far she has kept to her word. } } 在这个请求文件中，保存了即将发送给 Natural Language API 的文本的相关信息。type 属性的值可以是 PLAIN_TEXT 或 HTML 。content 中存放了将要发送给 Natural Language API 分析的文本。Natural Language API 还支持直接发送存储在 Google Cloud Storage 中的文件。直接从 Google Cloud Storage 发送文件时，请将 content 替换为 gcsContentUri ，并将其值设置为云端文件的 uri 地址。 调用 Natural Language API 现在，我们将使用 curl 命令，把请求文件和之前保存好的 API 密钥环境变量一起，发送给 Natural Language API （全放在一条命令里面）。 curl https://language.googleapis.com/v1/documents:analyzeEntities?key=${API_KEY} -s -X POST -H Content-Type: application/json --data-binary @request.json 你将得到形式如下的响应。 { entities: [  {  name: Harry Potter and the Deathly Hallows,  type: WORK_OF_ART,  metadata: {  mid: /m/03bkkv,  wikipedia_url: https://en.wikipedia.org/wiki/Harry_Potter_and_the_Deathly_Hallows  },  salience: 0.30040884,  mentions: [  {   text: {   content: book,   beginOffset: -1   },   type: COMMON  }  ]  } ], ... ... language: en } 在响应中，我们可以看到 API 从句子里检测到了6个实体（译者：原文是4个，但实际运行时得到6个，应该是API有所改善）。对于每个实体，你将得到实体的 type 、关联的维基百科URL（如果存在）、 salience （显著性）以及实体在文本中出现的位置的索引。 salience （显著性）是一个0~1的数字，指的是该实体对于整个文本的突出性。对于上述文段，「Harry Potter and the Deathly Hallows」具有最高的显著性（译者：原文是「Rowling」，应该是API有所改善），这是因为这部作品是文段所表述内容的主题。Natural Language API 也可以识别用其他方式表述的相同的实体，比如说「Rowling」、「J. K. Rowling」和「Joanne Kathleen Rowling」都指向同一个维基百科页面。 使用 Natural Language API 进行情感分析 除了提取实体，Natural Language API 还可以分析文本块的情感。JSON 请求文件与之前的那个具有相同的参数，但这次我们更改一下文本，换成一段具有更强烈情感的内容。请修改 request.json 为如下内容，或者换成你喜欢的文段。 request.json { document:{  type:PLAIN_TEXT,  content:I love everything about Harry Potter. It's the greatest book ever written. } } 然后把请求发送到 API 的 analyzeSentiment 端点。 curl https://language.googleapis.com/v1/documents:analyzeSentiment?key=${API_KEY} -s -X POST -H Content-Type: application/json --data-binary @request.json 你将得到形式如下的响应。 { documentSentiment: {  polarity: 1,  magnitude: 1.5,  score: 0.7 }, language: en, sentences: [  {  text: {  content: I love everything about Harry Potter.,  beginOffset: -1  },  sentiment: {  polarity: 1,  magnitude: 0.6,  score: 0.6  }  },  {  text: {  content: It's the greatest book ever written.,  beginOffset: -1  },  sentiment: {  polarity: 1,  magnitude: 0.8,  score: 0.8  }  } ] } 该方法将返回 polarity （极性）和 magnitude （强度）两个值（译者：原文是这两个值，但很明显现在还能看到 score 这个值）。polarity是介于-1.0 ~ 1.0之间的数值，表示文本消极或积极的程度。magnitude是介于0 ~ ∞的数值，与polarity没有关系，它表示在文本中表达的感情的权重。权重较大的文本块单是增加长度，其magnitude也会变大。上文的polarity是100%积极。「love」、「greatest」、「ever」这样的单词会影响magnitude的值。 分析语法与词性 让我们看看 Natural Language API 的第三个方法：文本注释。让我们进入文本的语言细节。annotateText方法提供了关于文本情感元素、语法元素的完整细节。使用该方法，可以知道文本中每个词语的词性（名词、动词、形容词等），以及各个单词如何与句子中的其他单词关联（是动词的原始形式，还是用来修饰语句）。 让我们通过简单的文段来使用这个方法。JSON 文件与之前的相似，但在这里我们需要添加一项 features 来告诉API你想要执行语法注释。请将request.json替换为如下内容。 request.json { document:{  type:PLAIN_TEXT,  content:Joanne Rowling is a British novelist, screenwriter and film producer. }, features:{  extractSyntax:true } } 然后把请求发送到 API 的 annotateText 端点。 curl https://language.googleapis.com/v1/documents:annotateText?key=${API_KEY} -s -X POST -H Content-Type: application/json --data-binary @request.json 响应中，对于句子中的每一个标记（token），会返回以下对象。 { text: {  tcontent: Joanne,  tbeginOffset: -1 }, partOfSpeech: {  ttag: NOUN,  taspect: ASPECT_UNKNOWN,  tcase: CASE_UNKNOWN,  tform: FORM_UNKNOWN,  tgender: GENDER_UNKNOWN,  tmood: MOOD_UNKNOWN,  tnumber: SINGULAR,  tperson: PERSON_UNKNOWN,  tproper: PROPER,  treciprocity: RECIPROCITY_UNKNOWN,  ttense: TENSE_UNKNOWN,  tvoice: VOICE_UNKNOWN }, dependencyEdge: {  theadTokenIndex: 1,  tlabel: NN }, lemma: Joanne }, 让我们详细看看返回值。从 partOfSpeech 可以看到「Joanne」是一个名词。 dependencyEdge 包含可用于创建依存句法分析树（依存構文木/Dependency-based parse trees）的数据。这个语法树是一个图表，用来显示句中各单词之间的关系。上述文段的依存句法分析树如下所示。  注：使用下面的 demo ，你可以在浏览器中创建自己的依存句法分析树。 https://cloud.google.com/natural-language/ 上述返回值中， headTokenIndex 是指具有指向「Joanne」的圆弧的标记（token）的索引。文段中的每一个标记（token）都可以看作是数组中的一个单词，「Joanne」的 headTokenIndex 值是1，表示依存句法分析树中连接了「Rowling」这个单词。 NN （修饰语句 noun compound （名词复合词）的略称）这个标签表示该词在改句子中的作用。「Joanne」是这个句子的主语「Rowling」的修饰词。 lemma 是这个单词的规范化形式。比如，run、runs、ran、running 这些单词的 lemma 都是 run 。lemma 有助于你调查大量文本中某一单词的出现频率。 其他语言的自然语言处理 Natural Language API 还支持其他很多语言的实体分析和语法注释。现在我们以日语为例，尝试进行日语文段的实体分析。 request.json { document:{  type:PLAIN_TEXT,  content:日本のグーグルのオフィスは、東京の六本木ヒルズにあります } } （译者：句意是“日本的谷歌办公大楼在东京的六本木新城。”） 我们不必告诉 API 这个文段是什么语言，API 能够自动检测出来。我们以相同的方式发送 API 请求。 curl https://language.googleapis.com/v1/documents:analyzeEntities?key=${API_KEY} -s -X POST -H Content-Type: application/json --data-binary @request.json 你将得到形式如下的响应。 { entities: [  {  name: 日本,  type: LOCATION,  metadata: {  wikipedia_url: https://en.wikipedia.org/wiki/Japan,  mid: /m/03_3d  },  salience: 0.23854347,  mentions: [  {   text: {   content: 日本,   beginOffset: -1   },   type: PROPER  }  ]  },  {  name: グーグル,  type: ORGANIZATION,  metadata: {  mid: /m/045c7b,  wikipedia_url: https://en.wikipedia.org/wiki/Google  },  salience: 0.21155767,  mentions: [  {   text: {   content: グーグル,   beginOffset: -1   },   type: PROPER  }  ]  },  ......  ...... ], language: ja } 恭喜！ 通过此次向导，我们尝试执行了实体提取、情感分析和语法注释，学会了如何使用 Natural Language API 进行文本分析。 学到的东西  构造 Natural Language API 请求，并使用 curl 发送请求 使用 Natural Language API 提取文本中的实体，并进行情感分析 使用 Natural Language API 对文本进行语言分析（语法、词性等） 使用不同的语言构造 Natural Language API 请求 下一步  浏览 Natural Language API 文档的向导。 尝试使用 Vision API 、Speech API 。 ",
      "url"      : "https://0qinghao.github.io/inforest/2018/04/07/google-natural-language-api/"
    } ,
  
    {
      "title"    : "在Windows命令行、Linux终端使用代理",
      "category" : "proxy",
      "content": "在之前的博文中分享了如何使用Google云计算引擎搭建SS服务器，如何使用SS客户端，已经满足了科学上网的基本需求。这次将要总结在Windows的 CMD 窗口和Linux的 LX终端 中，让 wget curl 等命令使用代理需要进行的一些配置。 Windows命令行代理 假设你已经使用了SS客户端，本地socks5代理为127.0.0.1:1080 在CMD窗口输入如下指令设置代理： set http_proxy=socks5://127.0.0.1:1080 set https_proxy=socks5://127.0.0.1:1080 set ftp_proxy=socks5://127.0.0.1:1080 测试 curl https://www.facebook.com 能得到返回结果。 取消代理命令： set http_proxy= set https_proxy= set ftp_proxy= 设置代理后只对当前命令行窗口生效，重新打开CDM需要再次设置。 Linux LX终端代理 由于Linux下SS客户端仅代理socks5协议的流量（如果不是这个原因恳请指正）。所以想在LX终端使用代理，需要在SS的socks5流量前再接一个代理，允许http、https、ftp协议流量通过。 我们也假定本地socks5代理为127.0.0.1:1080 安装polipo Debian下直接使用apt命令安装： sudo apt update sudo apt install polipo 编辑配置文件： sudo nano /etc/polipo/config 配置内容如下 # This file only needs to list configuration variables that deviate # from the default values. See /usr/share/doc/polipo/examples/config.sample # and polipo -v for variables you can tweak and further information. logSyslog = true logFile = /var/log/polipo/polipo.log proxyAddress = 0.0.0.0 socksParentProxy = 127.0.0.1:1080 socksProxyType = socks5 chunkHighMark = 50331648 objectHighMark = 16384 dnsQueryIPv6 = no 按CTRL+X，Y保存退出。 重启polipo服务： sudo service polipo restart 启用代理 通过 service polipo status 命令，我们可以看到新的监听端口为8123。 因此，LX终端启用代理的命令为： export http_proxy=http://127.0.0.1:8123 export https_proxy=http://127.0.0.1:8123 export ftp_proxy=http://127.0.0.1:8123 同样，直接输入上述命令设置的代理也是临时的。一个比较实用的方法是在~/.bashrc文件中设置环境，之后就不需要再手动设置了。 sudo nano ~/.bashrc 在文件最后插入上述三条指令，保存。 测试 wget 指令： 小结 我对CMD/LX终端设置代理的出发点，是为了使用Google的一个API，设置后确实能够成功使用。另外似乎对 pip 等指令也有效果，安装python模块时的下载速度有比较明显的提升。不过说到底只是在总结如何使用别人做好的工具，很多原理还是没有明白，如果文中有何纰漏，恳请指正。 感谢你阅读文章！ ",
      "url"      : "https://0qinghao.github.io/inforest/2018/09/19/proxy-set-in-windows-and-linux/"
    } ,
  
    {
      "title"    : "配置澎峰Perf-V开发板RISC-V开发环境",
      "category" : "risc-v",
      "content": "实验室买了一块Perf-V开发板，准备开始做RISC-V相关的工作。 虽然澎峰直接给了配置好的虚拟机开发环境，不过因为Ubuntu16.03用着不顺手就想要自己把环境配出来。 看澎峰给的SDK目录，就是直接用的开源蜂鸟（GitHub：e200_opensource）微调来的。所以最初尝试着git e200_opensource重新配置一遍，结果失败，报错找不到’cc1’。尝试添加PATH等操作无果。  riscv-none-embed-gcc: error trying to exec ‘cc1’: execvp: No such file or directory 后尝试直接把给的环境升级到18.04，结果出现与上述相同的问题。 那行吧，既然sirv-e-sdk和Perf-V-e-sdk都是从SIFIVE的freedom-e-sdk（GitHub：freedom-e-sdk）精简来的，那我就配置整个freedom-e-sdk吧。 总之这几天为了搞定Perf-V开发板的开发环境，前前后后踩了不少坑。现在把最后结果记录如下，备忘。 克隆freedom-e-sdk存储库 git clone --recursive https://github.com/sifive/freedom-e-sdk.git 文件大，耗时比较长。 从源代码构建Tools Ubuntu需要这些packages： sudo apt-get install autoconf automake libmpc-dev libmpfr-dev libgmp-dev gawk bison flex texinfo libtool libusb-1.0-0-dev make g++ pkg-config libexpat1-dev zlib1g-dev  build： cd freedom-e-sdk make tools [BOARD=freedom-e300-hifive1] build过程耗时很长。 替换板级支持包 freedom-e-sdk是SIFIVE的开发环境，里面的板级支持包只有sifive系列，要用来开发Perf-V需要先替换bsp文件夹。 mv ./bsp ./bsp_bak cp -r ~/fengniao/e200_opensource/Perf-V-e-sdk/bsp ./ 可以把Perf-V开发板自带的几个程序顺便复制过来，方便之后测试。 mv ./software ./software_bak cp -r ~/fengniao/e200_opensource/Perf-V-e-sdk/software ./ PC和开发板的连接 如果手头上有胡振波大大《RISC-V处理器》这本书的同学，请翻到P318，18.3节提到了他们的开发板是怎么配置和PC连接的。基本上照做就行了，可是别忘了，他们的开发板是Arty，虽然澎峰用的也是A7，但是板子ID可不一样，所以有2个参数要注意了。 我这就照着书上的步骤2到6简单写一下。 步骤二：通电；点USB图标连接至虚拟机 步骤三：使用如下命令查看USB状态 lsusb Bus 001 Device 002: ID 0403:6010 Future Technology Devices International, Ltd FT2232C Dual USB-UART/FIFO IC 记下 0403:6010 这两个数。 步骤四：设置udev rules，使USB能够被plugdev group访问 sudo nano /etc/udev/rules.d/99-openocd.rules # 写入以下内容，注意0403和6010，和书上不一样 SUBSYSTEM==usb, ATTR{idVendor}==0403, ATTR{idProduct}==6010, MODE=664, GROUP=plugdev SUBSYSTEM==tty, ATTRS{idVendor}==0403, ATTRS{idProduct}==6010, MODE=664, GROUP=plugdev 步骤五：看看USB设备所属组，略 步骤六：把自己的用户添加到组中 sudo usermod -a -G plugdev 你的用户名 编译上传裸机RISC-V程序 cd freedom-e-sdk make software PROGRAM=demo_gpio BOARD=Perf-V-creative-board make upload PROGRAM=demo_gpio BOARD=Perf-V-creative-board ",
      "url"      : "https://0qinghao.github.io/inforest/2018/10/30/per-f-risc-v-dev-env/"
    } ,
  
    {
      "title"    : "记一篇日语短文",
      "category" : "日语",
      "content": "今年（2019）7月的JLPT也快要开始了，我最重要的一个…emmm… 朋友，参加这次的N2考试，祝愿拿到好成绩。💪😊 两年前，大三的暑假，似乎比今年的夏天更加闷热一些，我第一次参加JLPT，也是N2。糊里糊涂刷了不到半本红蓝宝书，外加几套真题，就这样跑去另一个校区考试了。（考试前一天有点感冒，突然发烧头痛，好在睡了一下午就好了很多，想想自己当时也是心大😂） 那次的N2似乎比往年真题简单一些，接近3小时的答题时间过得还算舒适，本科3年各种突击应付式的考试都快让我忘记了这种——沉浸在试题里的紧张、兴奋和满足感。 那年的最后一篇长阅读（除去海报内容理解那篇）答得很开心，难度适中，文笔优美（按我的审美来说😂）。尽管是篇鸡汤文，但也强烈地让我想去拜读一下原作，可惜当时没能把出处记下来，也没办法Google到。最近不知为何又突然想起这篇文章，遂找出了当年的真题卷重温一遍，顺手在这做一下记录。 ​ t人生はいつも旅になぞらえられる。 ​ t人は人生という旅路を、地図もなく歩いている。誰しもそうだし、それが人間としては自然な姿である。人生に地図などあるわけがない。なのに人は、人生の地図を持とうとするのが常だ。暗闇の中を歩くのが不安で仕様がないのだ。迷ってしまった時の恐怖を想像したくないからだ。 ​ tそして自分の地図には、人生の設計図としてわがままな道程が記されている。三十歳までには結婚し、三十五歳頃には二人の子どもをもつ。四十歳には課長になり、五十歳までには何とか部長に昇進する。 ​ t（中略） ​ t人生の地図に描かれた道を、その通りに歩むことができるなら、そんなに楽なことはない。一度も脇道にそれずに、ただまっすぐに歩くことができるのなら、人は何も悩まなくても済むだろう。そんな人生を送る人間は、おそらくこの世に一人もいない。もしそういう人間がいるのだとしたら、それはその人間の人生ではない。その人生は他人から与えられたものに過ぎない。 ​ t五十歳の時には部長になっている。これは今という現在地から見た目標であろう。目標を持つことはもちろん大切なことだ。しかし、その目標へ辿り着く道は決して一本ではない。五十歳という現在地に立った時、もし部長になっていなければどうするのか。一枚の地図しか持っていない人、あるいは決して地図を書き変えようとしない人は、そこで人生の現在地を見失ってしまうだろう。｢今、自分はこの場所にいるはずなのに、全く違う所に来てしまった｣と、そんな思いに囚われてしまい、行くべき道も見失ってしまうのである。 ​ t地図を持たない人生が不安であるならば、地図を持てばいいだけのことだ。しかし、その一枚の地図にこだわってはならない。常に現在地を確認しながら、どんどん地図を変えていくことだ。 ​ t少し脇道に入ってしまったのなら、その脇道を歩いてみればいい。無理をして元の道に戻ろうとしても、余計に迷うだけだ。脇道を歩いているうちに、いつの間にか元の道に戻ることもあるだろうし、また別の大通りに出会うこともあるだろう。人生には数え切れないほどの道があることを知っておいたほうがいい。今いる場所さえしっかりと認識できていれば、人はどんな道だって歩いていくことができるものだ。 （立松平和『人生の現在地—まだまだ迷っているぞ、私は。』による） ",
      "url"      : "https://0qinghao.github.io/inforest/2019/06/25/2017-07-jlpt-n2/"
    } ,
  
    {
      "title"    : "译 - 使用iStyle格式化Verilog代码",
      "category" : "verilog",
      "content": " 原文：Verilogでコード整形 安装 iStyle可以从GitHub上clone、make自行编译出可执行文件，也可以直接下载已编译好的可执行文件。这里都给出来。 Github https://github.com/thomasrussellmurphy/istyle-verilog-formatter 可执行文件 https://github.com/HayasiKei/istyle-verilog-formatter/releases/tag/v1.21_x86_64 格式化选项 以下是一些格式化时常用的选项及效果示例。 待格式化代码 reg [3:0] cnt; always @(posedge clk or posedge rst) begin if(rst) begin cnt&lt;=4'h0; end else begin cnt&lt;=cnt+4'h1; end end –style ANSI style ./iStyle --style=ansi test.v reg [3:0] cnt; always @(posedge clk or posedge rst) begin  if(rst)  begin  cnt&lt;=4'h0;  end  else  begin  cnt&lt;=cnt+4'h1;  end end  Kernighan&amp;Ritchie style ./iStyle --style=kr test.v reg [3:0] cnt; always @(posedge clk or posedge rst) begin  if(rst) begin  cnt&lt;=4'h0;  end  else begin  cnt&lt;=cnt+4'h1;  end end  GNU style ./iStyle --style=gnu test.v reg [3:0] cnt; always @(posedge clk or posedge rst) begin  if(rst)  begin  cnt&lt;=4'h0;  end  else  begin  cnt&lt;=cnt+4'h1;  end end -s ./iStyle -s2 test.v 该选项指定缩进时的空格数量，-s2表示每次缩进使用2个空格（如果是-s4则表示每次用4个空格缩进）。 reg [3:0] cnt; always @(posedge clk or posedge rst) begin if(rst) begin  cnt&lt;=4'h0; end else begin  cnt&lt;=cnt+4'h1; end end -p -p选项指定在运算符两侧插入空格。 reg [3: 0] cnt; always @(posedge clk or posedge rst) begin  if (rst)  begin  cnt &lt;= 4'h0;  end else  begin  cnt &lt;= cnt + 4'h1;  end end -P -P选项指定在运算符和括号周围插入空格。 reg [ 3: 0 ] cnt; always @( posedge clk or posedge rst ) begin  if ( rst )  begin  cnt &lt;= 4'h0;  end else  begin  cnt &lt;= cnt + 4'h1;  end end 小结 虽然文中没有写，module声明的缩进感觉并不是很好。verilog有各种各样的代码风格，因此有一个强大的格式化程序是很有用的。 ",
      "url"      : "https://0qinghao.github.io/inforest/2019/08/09/translate-use-istyle-to-format-verilog-code/"
    } ,
  
    {
      "title"    : "Wiki Template",
      "category" : "cate1",
      "content": "Content here ",
      "url"      : "https://0qinghao.github.io/inforest/wiki/template/"
    } 
  
]

